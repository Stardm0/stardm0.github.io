[{"title":"修复webrtc示例中peer_connection_client在windows上运行的问题","url":"/2025/05/13/修复webrtc示例中peer_connection_client运行在windows上运行的问题/","content":"\n想学习一下webrtc原生API的使用，开始是学习 https://www.nxrte.com/jishu/webrtc/29168.html 这个项目的，奈何出现太多问题了，webtc版本问题、线程安全问题、好些问题都不知道什么原因造成的、最后就是弄到能跑一半的程度。只能说，还得学(￣﹃￣)，然后就是看到官方源码上的案例，但就是挺坑的，运行还有点问题。在网上各处找解决办法。总结下来，要改的地方有三处。\n\n\n### 需要修改的地方\n\n#### 第一处在main.cc中\n\n```c++\n  MainWnd wnd(server.c_str(), absl::GetFlag(FLAGS_port),\n              absl::GetFlag(FLAGS_autoconnect), absl::GetFlag(FLAGS_autocall));\n  if (!wnd.Create()) {\n    RTC_DCHECK_NOTREACHED();\n    return -1;\n  }\n\n  rtc::InitializeSSL();\n  PeerConnectionClient client;\n  auto conductor = rtc::make_ref_counted<Conductor>(&client, &wnd);\n  // Start loop to handle callback of socket events\n  main_thread.Start(); // 添加这一行\n\n  // Main loop.\n  MSG msg;\n  BOOL gm;\n  while ((gm = ::GetMessage(&msg, NULL, 0, 0)) != 0 && gm != -1) {\n    if (!wnd.PreTranslateMessage(&msg)) {\n      ::TranslateMessage(&msg);\n      ::DispatchMessage(&msg);\n    }\n  }\n```\n\n#### 第二处在peer_connection_client.cc中\n\n```c++\n\n  state_ = NOT_CONNECTED;\n}\nbool PeerConnectionClient::ConnectControlSocket() {\n  RTC_DCHECK(control_socket_->GetState() == rtc::Socket::CS_CLOSED);\n  int err = control_socket_->Connect(server_address_);\n  if (err == SOCKET_ERROR) {\n    Close();\n    return false;\n  }\n\n// 添加以下预处理指令 用于在 Windows 平台唤醒 socket server 的线程\n#if defined(WEBRTC_WIN)\n  rtc::Thread::Current()->socketserver()->WakeUp();\n#endif\n  return true;\n\n}\nvoid PeerConnectionClient::OnConnect(rtc::Socket* socket) {\n  RTC_DCHECK(!onconnect_data_.empty());\n  size_t sent = socket->Send(onconnect_data_.c_str(), onconnect_data_.length());\n  RTC_DCHECK(sent == onconnect_data_.length());\n  onconnect_data_.clear();\n}\n\n```\n\n#### 第三处在源文件的sink_filter_ds.cc中\n\n```c++\nCaptureInputPin::Receive(IMediaSample* media_sample) {\n  //RTC_DCHECK_RUN_ON(&capture_checker_); //注释掉这一行\n\n  // RTC_DCHECK_RUN_ON是检查线程一致的，在我的电脑中，我发现视像头的捕获线程不断发生变化，无法保持线程的一致性，导致断言错误。\n\n  ....\n}\n```","tags":["WebRtc"],"categories":["音视频"]},{"title":"webrtc编译","url":"/2025/04/14/webrtc编译/","content":"\n为了使用编译后的webrtc库我花了很多时间，终于在今天解决了各个问题，并正式在自己的项目中使用。说真的很累^(*￣(oo)￣)^。但成功了让我觉得这一切都没有白费(≧▽≦q)\n\n**Windows 环境下下载 WebRTC 源码**\n\n---\n\n## 准备工作\n\n### 安装必要工具\n\n#### Visual Studio\n- 安装 **Visual Studio 2022**\n- 勾选以下组件：\n  - C++开发桌面工作负载（包含 MSVC、Windows 10 SDK）\n\n---\n\n### 安装 Python\n> 安装时勾选 “Add Python to PATH”。\n\n---\n\n### 安装 Git\n从官网下载并安装：[https://git-scm.com/download/win](https://git-scm.com/download/win)\n\n假设你的代理地址是：127.0.0.1:7890\n```bash\ngit config --global http.proxy http://127.0.0.1:7890\ngit config --global https.proxy http://127.0.0.1:7890\n```\n\n---\n\n## 下载 `depot_tools`并配置环境\n\n```bash\ngit clone https://chromium.googlesource.com/chromium/tools/depot_tools.git\n```\n\n### 配置环境变量：\n- 将 `depot_tools` 所在目录添加到系统的 `PATH`\n  - 控制面板 → 系统 → 环境变量 → `Path` → 添加一行：`C:\\your\\path\\to\\depot_tools`\n- 添加系统变量 \n- 编译相关\nDEPOT_TOOLS_UPDATE=0 \nDEPOT_TOOLS_WIN_TOOLCHAIN=0 \nGYP_MSVS_OVERRIDE_PATH=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community 这个变量和生成vs工程有关，就是在编译参数添加的--ide=vs2022\n\n- 网络相关 \nhttp_proxy=你的代理地址\nhttps_proxy=你的代理地址\nNO_AUTH_BOTO_CONFIG=/youpath/.boto\n\n.boto文件的内容如下\n```txt\n[Boto]\nproxy = 127.0.0.1\nproxy_port = 7890\nhttps_proxy = 127.0.0.1\nhttps_proxy_port = 7890\n```\n\n---\n\n## 拉取 WebRTC 源码\n\n### 打开 **x64 Native Tools Command Prompt for VS**（推荐使用这个终端）\n\n\n### 创建工作目录\n\n```powershell\nmkdir webrtc-checkout\ncd webrtc-checkout\n```\n\n### 拉取源码（首次使用）\n```powershell\nfetch --nohooks webrtc\n```\n\n⚠️ 第一次会下载 Chromium 构建环境，时间较长。\n\n---\n\n### 进入源码目录并同步依赖\n\n```powershell\ncd src\ngclient sync\n```\n\n---\n\n## 生成和构建\n\n### 创建构建目录\n```powershell\ngn gen out/Default\n```\n\n在out/Default/args.gn文件中\n设置如下编译参数：\n\n```txt\n# Set build arguments here. See `gn help buildargs`.\ntarget_os = \"win\"\ntarget_cpu = \"x64\"\nis_debug = true\nis_clang = true\nuse_custom_libcxx = false\n```\nis_debug = true 控制是否是调试版本的\n\n### 编译（使用 Ninja）\n```powershell\nninja -C out/Default --ide=vs2022\n```\n编译过程中可能出现少量语法问题，可通过询问AI手动解决\n\n---\n\n## 在VS项目中使用webrtc库文件\n\n需要添加以下预编译宏定义\nWEBRTC_WIN\nNOMINMAX\nRTC_ENABLE_WIN_WGC\n\n采用调试还要添加以下这行\n_ITERATOR_DEBUG_LEVEL=0\n","tags":["WebRtc"],"categories":["音视频"]},{"title":"进程与线程(2)","url":"/2025/03/17/进程与线程(2)/","content":"\n## 线程管理\n\n### 线程控制\n\n#### 线程创建\n\n通过`pthread_create`函数创建线程，线程可以并发执行任务。示例中创建了两个线程，一个用于读取控制台输入，另一个用于将数据输出到控制台。\n\n```c\n#include<pthread.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#define BUF_LEN 1024\n\nchar *buf;\n\nvoid *input_thread(void *argv){\n    int i = 0;\n    while(1){\n        char c = fgetc(stdin);\n        if(c && c != '\\n'){\n            buf[i++] = c;\n        }\n        if(i >= BUF_LEN){\n            i = 0;\n        }\n    }\n}\n\nvoid *output_thread(void *argv){\n    int i = 0;\n    while(1){\n        if(buf[i]){\n            fputc(buf[i], stdout);\n            fputc('\\n', stdout);\n            buf[i++] = 0;\n            if(i >= BUF_LEN){\n                i = 0;\n            }\n        }else{\n            sleep(1);\n        }\n    }\n}\n\n// 创建两个线程 一个读取控制台数据 一个将数据写出控制台\n\n\n\nint main(int argc, char const *argv[])\n{\n    pthread_t pid_input;\n    pthread_t pid_output;\n\n    buf = (char *)malloc(sizeof(char) * BUF_LEN);\n\n    // 创建读线程\n    pthread_create(&pid_input, NULL, input_thread, NULL);\n\n    //创建写线程\n    pthread_create(&pid_output, NULL, output_thread, NULL);\n\n    //主线线程等待子线程结束\n    pthread_join(pid_input, NULL);\n    pthread_join(pid_output, NULL);\n\n    free(buf);\n\n    return 0;\n}\n\n```\n\n### 线程终止\n\n线程可以通过`pthread_exit`或返回函数值自然终止，也可以通过`pthread_cancel`强制终止。`pthread_setcanceltype`可以设置取消类型为异步或延迟。\n\n#### detach不挂起终止\n\n使用`pthread_detach`标记线程为分离状态，线程结束后自动回收资源，主线程无需等待。\n```c\n#include<pthread.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n\nvoid *task(void *arg){\n    printf(\"thread started\\n\");\n    sleep(2);\n    printf(\"thread finished\\n\");\n    return NULL;\n}\n\nint main(int argc, char const *argv[])\n{\n    pthread_t tid;\n    pthread_create(&tid, NULL, task, NULL);\n\n    // 使用detach标记 等线程完成后回收相关资源\n    pthread_detach(tid);\n\n    // 主线程运行完成比创建的线程慢\n    printf(\"main thread continues\\n\");\n    sleep(3); //因为主线程不会等待 如果父线程先结束会强制杀死子进程\n    printf(\"main thread ending\");\n    return 0;\n}\n\n\n```\n\n#### cancel的延迟取消\n\n通过`pthread_cancel`发送取消请求，线程在遇到取消点（如sleep或pthread_testcancel）时才会终止。\n\n```c\n#include<pthread.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n\nvoid *task(void *arg){\n    printf(\"thread started\\n\");\n    //默认取消类型是延迟\n    printf(\"working...\");\n    sleep(1); //sleep()会检测是否有取消请求标记\n    //人为调用取消点函数\n    pthread_testcancel();\n    printf(\"thread finished\\n\");\n    return NULL;\n}\n\nint main(int argc, char const *argv[])\n{\n    pthread_t tid;\n    pthread_create(&tid, NULL, task, NULL);\n\n    // 取消子线程\n    if (pthread_cancel(tid) != 0)\n    {\n        perror(\"pthread_cancel\");\n    }\n    void *res;\n    // pthread_cancel只是发出一个停止命令\n    pthread_join(tid, &res);\n    if(res == PTHREAD_CANCELED){\n        printf(\"线程被取消\\n\");\n    }else{\n        printf(\"线程没有被取消 exit code %ld\\n\", (long)res);\n    }\n\n    return 0;\n}\n\n\n```\n\n#### cancel的异步取消和禁用取消\n\n`pthread_setcancelstate`可以禁用或启用线程的取消响应，`pthread_setcanceltype`设置取消类型为异步。\n\n\n```c\n#include<pthread.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n\nvoid *task(void *arg){\n    //printf(\"thread started, thread id: %lu\\n\", pthread_self());\n    int oldstate;\n\n    //fflush(stdout); // 确保输出立即刷新\n    //printf(\"cancel state disabled, old state: %d\\n\", oldstate);\n    // 禁用取消响应\n    //pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, &oldstate);\n\n    printf(\"working...\\n\");\n    fflush(stdout); // 确保输出立即刷新\n\n    sleep(1); // sleep() 会检测是否有取消请求标记\n\n    printf(\"cancel state check before re-enable, old state: %d\\n\", oldstate);\n    fflush(stdout); // 确保输出立即刷新\n\n    // 恢复取消状态\n    pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, &oldstate);\n    printf(\"cancel state enabled, restored old state: %d\\n\", oldstate);\n    fflush(stdout); // 确保输出立即刷新\n\n    printf(\"thread finished\\n\");\n    fflush(stdout); // 确保输出立即刷新\n\n    return NULL;\n}\n\nint main(int argc, char const *argv[])\n{\n    pthread_t tid;\n    pthread_create(&tid, NULL, task, NULL);\n    // 取消子线程\n    if (pthread_cancel(tid) != 0)\n    {\n        perror(\"pthread_cancel\");\n    }\n    void *res;\n    // pthread_cancel 只是发出一个停止命令\n    pthread_join(tid, &res);\n    if(res == PTHREAD_CANCELED){\n        printf(\"线程被取消\\n\");\n    }else{\n        printf(\"线程没有被取消 exit code %ld\\n\", (long)res);\n    }\n\n    return 0;\n}\n\n```\n\n### 线程同步\n\n#### 竞态条件与互斥锁\n\n多个线程同时访问共享资源可能导致竞态条件。使用互斥锁（`pthread_mutex_lock`和`pthread_mutex_unlock`）确保同一时间只有一个线程访问共享资源。\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#include<pthread.h>\n\n#define THREAD_COUNT 30000\n\n// 初始化锁\nstatic pthread_mutex_t counter_mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid *add_thread(void *arg){\n    int *p = (int *)arg;\n    // 在累加之前获取锁 保证只有一个线程进行累加\n    pthread_mutex_lock(&counter_mutex);\n    (*p)++;\n    // 累加之后释放锁\n    pthread_mutex_unlock(&counter_mutex);\n    return NULL;\n}\n\nint main(int argc, char const *argv[])\n{\n    pthread_t pid[THREAD_COUNT];\n    int num = 0;\n    for(size_t i = 0; i < THREAD_COUNT; i++){\n        //创建的线程功能是给传入的参数加一\n        pthread_create(pid + i, NULL, add_thread, &num);\n    }\n\n    for (size_t i = 0; i < THREAD_COUNT; i++)\n    {\n        pthread_join(pid[i], NULL);\n    }\n    \n    printf(\"累加结果%d\\n\", num);\n    return 0;\n}\n\n```\n\n#### 读写锁\n\n读写锁（`pthread_rwlock`）允许多个线程同时读取共享资源，但写操作需要独占访问，适合读多写少的场景。\n\n```c\n#include<unistd.h>\n#include<stdio.h>\n#include<pthread.h>\n\n//static pthread_rwlock_t rwlock = __PTHREAD_RWLOCK_INITIALIZER;\npthread_rwlock_t rwlock;\nint shared_date = 0;\n\nvoid *lock_write(void *arg){\n    \n    pthread_rwlock_wrlock(&rwlock);\n    int tmp = shared_date + 1;\n    sleep(1);\n    shared_date = tmp;\n    pthread_rwlock_unlock(&rwlock);\n    printf(\"但前是%s, shared_date为%d\\n\", (char *)arg, shared_date);\n}\n\nvoid *lock_read(void *arg){\n    // 读写锁中的度是可以由多个线程统一读取的\n    //获取读锁\n    pthread_rwlock_rdlock(&rwlock);\n    printf(\"当前是%s,shared_date为%d\\n\", (char*)arg, shared_date);\n    pthread_rwlock_unlock(&rwlock);\n    \n}\n\nint main(int argc, char const *argv[])\n{\n    //动态初始化读写锁\n    pthread_rwlock_init(&rwlock, NULL);\n\n    pthread_t write1, write2, reader1, reader2, reader3, reader4, reader5, reader6;\n\n    //创建两个写线程\n    pthread_create(&write1, NULL, lock_write, \"write1\");\n    pthread_create(&write2, NULL, lock_write, \"write2\");\n\n    // 休眠等待\n    sleep(3);\n\n    pthread_create(&reader1, NULL, lock_read, \"reader1\");\n    pthread_create(&reader2, NULL, lock_read, \"reader2\");\n    pthread_create(&reader3, NULL, lock_read, \"reader3\");\n    pthread_create(&reader4, NULL, lock_read, \"reader4\");\n    pthread_create(&reader5, NULL, lock_read, \"reader5\");\n    pthread_create(&reader6, NULL, lock_read, \"reader6\");\n\n    //主线程等待创建的子线程运行完成\n    pthread_join(write1, NULL);\n    pthread_join(write2, NULL);\n    pthread_join(reader1, NULL);\n    pthread_join(reader2, NULL);\n    pthread_join(reader3, NULL);\n    pthread_join(reader4, NULL);\n    pthread_join(reader5, NULL);\n    pthread_join(reader6, NULL);\n\n\n    //销毁读写锁\n    pthread_rwlock_destroy(&rwlock);\n    return 0;\n\n}\n\n```\n\n#### 读写锁的写饥饿解决\n\n通过设置读写锁属性（`pthread_rwlockattr_setkind_np`）为写优先，避免写线程因大量读线程而长时间等待。\n\n```c\n#include<unistd.h>\n#include<stdio.h>\n#include<pthread.h>\n\n//static pthread_rwlock_t rwlock = __PTHREAD_RWLOCK_INITIALIZER;\npthread_rwlock_t rwlock;\nint shared_date = 0;\n\nvoid *lock_write(void *arg){\n    \n    printf(\"我%s要获取写锁\\n\", (char *)arg);\n    pthread_rwlock_wrlock(&rwlock);\n    int tmp = shared_date + 1;\n    shared_date = tmp;\n    printf(\"当前是%s, shared_date为%d\\n\", (char *)arg, shared_date);\n    pthread_rwlock_unlock(&rwlock);\n    printf(\"%s释放了写\\n锁\", (char*)arg);\n}\n\nvoid *lock_read(void *arg){\n    // 读写锁中的度是可以由多个线程统一读取的\n    //获取读锁\n    printf(\"我%s要获取读锁\\n\", (char*)arg);\n    pthread_rwlock_rdlock(&rwlock);\n    printf(\"当前是%s,shared_date为%d\\n\", (char*)arg, shared_date);\n    sleep(1);\n    pthread_rwlock_unlock(&rwlock);\n    printf(\"%s释放了读锁\\n\", (char*)arg);\n    \n}\n\nint main(int argc, char const *argv[])\n{\n    //创建读写锁属性对象\n    pthread_rwlockattr_t attr;\n    pthread_rwlockattr_init(&attr);\n    //修改对象属性 设置写优先\n    pthread_rwlockattr_setkind_np(&attr, PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP);\n    //动态初始化读写锁\n    pthread_rwlock_init(&rwlock, &attr);\n\n    pthread_t write1, write2, reader1, reader2, reader3, reader4, reader5, reader6;\n\n    //创建两个写线程\n    pthread_create(&write1, NULL, lock_write, \"write1\");\n\n    pthread_create(&reader1, NULL, lock_read, \"reader1\");\n    pthread_create(&reader2, NULL, lock_read, \"reader2\");\n    pthread_create(&reader3, NULL, lock_read, \"reader3\");\n    // 在线程读取的中间获取一个写锁的操作\n    pthread_create(&write2, NULL, lock_write, \"write2\");\n    pthread_create(&reader4, NULL, lock_read, \"reader4\");\n    pthread_create(&reader5, NULL, lock_read, \"reader5\");\n    pthread_create(&reader6, NULL, lock_read, \"reader6\");\n\n    //主线程等待创建的子线程运行完成\n    pthread_join(write1, NULL);\n    pthread_join(write2, NULL);\n    pthread_join(reader1, NULL);\n    pthread_join(reader2, NULL);\n    pthread_join(reader3, NULL);\n    pthread_join(reader4, NULL);\n    pthread_join(reader5, NULL);\n    pthread_join(reader6, NULL);\n\n\n    //销毁读写锁\n    pthread_rwlock_destroy(&rwlock);\n    return 0;\n\n}\n\n```\n\n#### 自旋锁\n\n**自旋锁（Spinlock）**是一种用于多线程或多核环境下保护共享资源的同步机制。当线程尝试获取锁时，如果锁已被占用，线程会进入忙等待状态（自旋），不断检查锁是否被释放，而不是进入阻塞状态。自旋锁适用于锁持有时间较短的场景，避免线程切换的开销，但长时间自旋会浪费CPU资源。\n\n#### 条件变量\n\n条件变量（`pthread_cond_wait`和`pthread_cond_signal`）用于线程间通信，允许线程在特定条件满足时唤醒其他线程。\n\n```c\n#include<stdio.h>\n#include<stdlib.h>\n#include<pthread.h>\n\n#define BUFFER_SIZE 5\nint buffer[BUFFER_SIZE];\nint count = 0;\n\n// 初始化互斥锁\nstatic pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n\n// 初始化条件变量\nstatic pthread_cond_t cond = PTHREAD_COND_INITIALIZER;\n\nvoid * producer(void *arg){\n    int item = 1;\n    //获取互斥锁\n    pthread_mutex_lock(&mutex);\n    while(1){\n        // 如果缓冲区写满 使用条件变量暂时释放当前线程\n        if(count == BUFFER_SIZE){\n            //唤醒消费者\n            pthread_cond_signal(&cond);\n            pthread_cond_wait(&cond, &mutex);\n        }\n        //缓冲区没有满\n        buffer[count++] = item++;\n        printf(\"白月光发送了一个幸运数字%d\\n\", buffer[count - 1]);\n\n    }\n    //释放\n    pthread_mutex_unlock(&mutex);\n}\nvoid * consumer(void *arg){\n    //获取互斥锁\n    pthread_mutex_lock(&mutex);\n    while(1){\n        if(count == 0){\n            //唤醒生产者\n            pthread_cond_signal(&cond);\n            //缓冲中没有消息可读\n            pthread_cond_wait(&cond, &mutex);\n        }\n        printf(\"我收到白月光的幸运数字%d\\n\", buffer[--count]);\n    }\n    //释放\n    pthread_mutex_unlock(&mutex);\n}\n\nint main(int argc, char const *argv[])\n{\n    // 创建两个线程\n    pthread_t producer_thread, consumer_thread;\n\n    pthread_create(&producer_thread, NULL, producer,NULL);\n    pthread_create(&consumer_thread, NULL, consumer,NULL);\n\n    pthread_join(producer_thread, NULL);\n    pthread_join(consumer_thread, NULL);\n\n    return 0;\n}\n\n```\n\n\n### 线程池\n\n线程池是一种并发编程技术，通过预先创建一组线程并复用它们来执行任务，避免了频繁创建和销毁线程的开销。线程池的核心思想是将任务提交到任务队列中，由池中的空闲线程从队列中取出任务并执行。它适用于需要处理大量短任务的场景，能够提高程序性能、降低资源消耗，并简化线程管理。常见的线程池实现包括固定大小线程池、动态调整线程池等。\n\n```c\n#include<glib.h>\n#include<stdio.h>\n#include<unistd.h>\n#include<stdlib.h>\n\nvoid task_func(gpointer data, gpointer user_data){\n    int task_num = *(int *)data;\n    free(data);\n    printf(\"开始执行%d任务\\n\", task_num);\n    sleep(task_num);\n    printf(\"%d任务执行完成\\n\", task_num);\n}\n\nint main(int argc, char const *argv[])\n{\n    //创建线程池\n    GThreadPool *pool = g_thread_pool_new(task_func, NULL, 5, TRUE, NULL);\n\n    //向线程池中添加任务\n    for(int i = 0; i < 10; i++){\n        //每一个任务的编号\n        int *tmp = (int*)malloc(sizeof(int));\n        *tmp = i + 1;\n\n        g_thread_pool_push(pool, tmp, NULL);\n    }\n\n    g_thread_pool_free(pool, FALSE, TRUE);\n    printf(\"所有的任务都完成了\\n\");\n    return 0;\n}\n\n```","tags":["Linux","C"],"categories":["Linux应用层"]},{"title":"进程与线程(1)","url":"/2025/03/13/进程与线程(1)/","content":"\n这里我将学习的进程与线程分为三个大部分\n\n1.进程创建：使用fork创建子进程，execve执行新程序。\n2.​进程通信：匿名管道、有名管道、共享内存、消息队列、信号\n​3.线程管理：线程创建、终止、同步（互斥锁、读写锁、条件变量等）\n要想进一步了解系统的底层原理可以去看操作系统相关的知识，了解了解进程线程的内存模型，以及进程线程在系统上的工作原理。这里有一篇不错的文章\nhttps://zybtree.github.io/2020/08/10/%E8%BF%9B%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/\n\n\n## 进程创建\n\n### 使用 `fork` 创建子进程\n• **`fork` 的作用**：`fork` 是 Linux 系统中用于创建子进程的系统调用。调用 `fork` 后，父进程会复制一份自己的地址空间给子进程，子进程从 `fork` 返回处开始执行。这里涉及到`写时复制`\n• **返回值**：\n  • `-1`：创建失败。\n  • `0`：在子进程中返回。\n  • `> 0`：在父进程中返回子进程的 PID。\n• **代码示例**：\n  ```c\n#include<stdio.h>\n#include<sys/types.h>\n#include<unistd.h>\n\nint main(int argc, char *argv[]){\n    //调用fork之前 代码都在父进程中运行\n    printf(\"当前%d\\n\", getpid());\n\n    /**不需要传参\n     * return: int 进程号\n     *        (1):-1 出错\n     *        (2): 父进程中表示子进程的PID\n     *        (3): 子进程中显示为0\n     * __pid_t fork (void)\n     */\n    //使用fork创建子进程 fork之后所有的代码都是在父子进程中各自执行一遍\n    pid_t pid = fork();\n    \n    //printf(\"%d\\n\", pid);\n    if(pid < 0){\n        printf(\"子进程创建失败!\");\n        return 1;\n    }else if(pid == 0){\n        //当前为子进程\n        printf(\"子进程 %d 创建成功! 父进程是 %d\\n\", getpid(), getppid());\n    }else{\n        //当前为父进程\n        printf(\"当前为父进程 %d 子进程是 %d\\n\", getpid(), pid);\n    }\n    return 0;\n}\n  ```\n\n### 使用 `execve` 执行新程序\n• **`execve` 的作用**：`execve` 用于替换当前进程的地址空间为新的程序，并执行该程序。调用成功后，原程序的代码和数据被新程序替换。这也意味着该进程的pid保持不变\n• **代码示例**：\n  ```c\n#include<stdio.h>\n#include<unistd.h>\n\nint main(int argc, char *argv[]){\n    //跳转之前\n    char *name = \"banzhang\";\n    printf(\"我是%s 编号%d\\n\", name, getpid());\n\n    //执行跳转\n    /**\n     * const char *__path: 执行程序的路径\n     * char *const __argv[]：传入参数 -> 对应执行程序main方法的第二个参数\n     *      (1): 第一个参数固定是程序的名称 -> 执行的程序路径\n     *      (2): 执行程序要传入的参数\n     *      (3): 结尾的最后一个参数一定是NULL\n     * char *const __envp[]: 传递的环境变量\n     *      (1): 环境变量参数: key=value\n     *      (2): 最后一个参数为NULL\n     * return: 成功根本没法返回 下面的代码也没有意义 失败返回-1;\n     * int execve (const char *__path, char *const __argv[], char *const __envp[])\n     */\n    char *args[] = {\"/home/stream/process_test/erlou\", name, NULL};\n    char *envs[] = {\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin\",NULL};\n    int re = execve(args[0], args, envs);\n    if(re == -1){\n        printf(\"调用程序失败!\\n\");\n        return 1;\n    }\n\n    return 0;\n}\n  ```\n\n---\n\n## 进程通信\n\n### 匿名管道\n• **匿名管道的特点**：半双工通信，只能用于有亲缘关系的进程间通信。同时保持一个读一个写的原则\n• **代码示例**：\n  ```c\n#include<sys/types.h>\n#include<sys/wait.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#include<string.h>\n\nint main(int argc, char *argv[]){\n    pid_t cpid;\n    int pipefd[2];\n\n    //将程序传近来的第一个命令行参数 通过管道传输给子进程\n    if(argc != 2){\n        fprintf(stderr, \"%s:请填写需要传递的信息!\\n\", argv[0]);\n        exit(EXIT_FAILURE);\n    }\n\n    //创建管道\n    if(pipe(pipefd) == -1){\n        perror(\"创建管道失败\");\n        exit(EXIT_FAILURE);\n    }\n\n    //复制父子进程\n    cpid = fork();\n    if(cpid == -1){\n        perror(\"邀请新学员失败\");\n        exit(EXIT_FAILURE);\n    }\n    if(cpid == 0){\n        //子进程 读取管道的数据 打印到控制台\n        //write(pipefd[1], argv[1], strlen(argv[1]));\n        close(pipefd[1]);\n        char str[100] = {0};\n        sprintf(str, \"新学员 %d接受信息\\n\", getpid());\n        write(STDOUT_FILENO, &str, sizeof(str));\n        char buf;\n        while(read(pipefd[0], &buf, 1) > 0){\n            write(STDOUT_FILENO, &buf, 1);\n        }\n        write(STDOUT_FILENO, \"\\n\", 1);\n        close(pipefd[0]);\n        _exit(EXIT_SUCCESS);\n    }else{\n        //父进程 写入管道数据 打印到控制台\n        //close(pipefd[0]);\n        // 将数据写入\n        printf(\"老学员%d对新学员传递信息\\n\", getpid());\n        write(pipefd[1], argv[1], strlen(argv[1]));\n        close(pipefd[1]);\n        waitpid(cpid, NULL, 0);\n        // char buf;\n        // while(read(pipefd[0], &buf, 1) > 0){\n        //     write(STDOUT_FILENO, &buf, 1);\n        // }\n        // write(STDOUT_FILENO, \"\\n\", 1);\n        close(pipefd[0]);\n        exit(EXIT_SUCCESS);\n    }\n    return 0;\n}\n  ```\n\n### 有名管道\n• **有名管道的特点**：允许无亲缘关系的进程间通信，通过文件系统中的特殊文件实现。\n• **写进程代码示例**：\n  ```c\n#include<sys/stat.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#include<fcntl.h>\n#include<string.h>\n#include<errno.h>\n\nint main(int argc, char *argv[]){\n    int fd;\n    char *pipe_path = \"/tmp/myfifo\";\n    if(mkfifo(pipe_path, 0664) != 0){\n        perror(\"mkfifo\");\n        exit(EXIT_FAILURE);\n    }\n    //对有名管道的特殊文件 创建fd\n    fd = open(pipe_path, O_WRONLY);\n    if(fd == -1){\n        perror(\"open\");\n        exit(EXIT_FAILURE);\n    }\n    char buf[100];\n    size_t read_num;\n    while((read_num = read(STDIN_FILENO, buf, 100)) > 0){\n        write(fd, buf, read_num);\n    }\n    if(read_num < 0){\n        perror(\"read\");\n        close(fd);\n        exit(EXIT_FAILURE);\n    }\n\n    printf(\"发送数据到管道完成 进程终止\");\n    close(fd);\n    if(unlink(pipe_path) == -1){\n        perror(\"unlink\");\n    }\n\n    return 0;\n}\n  ```\n\n• **读进程代码示例**：\n  ```c\n#include<sys/stat.h>\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#include<fcntl.h>\n#include<string.h>\n#include<errno.h>\n\nint main(int argc, char *argv[]){\n    int fd;\n    char *pipe_path = \"/tmp/myfifo\";\n\n    //对有名管道的特殊文件 创建fd\n    fd = open(pipe_path, O_RDONLY);\n    if(fd == -1){\n        perror(\"open\");\n        exit(EXIT_FAILURE);\n    }\n    char buf[100];\n    size_t read_num;\n    while((read_num = read(fd, buf, 100)) > 0){\n        write(STDOUT_FILENO, buf, read_num);\n    }\n\n    if(read_num < 0){\n        perror(\"read\");\n        close(fd);\n        exit(EXIT_FAILURE);\n    }\n\n    printf(\"接受管道数据完成 进程终止\");\n    close(fd);\n\n    return 0;\n}\n  ```\n\n### 共享内存\n• **共享内存的特点**：多个进程共享同一块内存区域，是最快的进程间通信方式。\n• **代码示例**：\n  ```c\n#include<stdio.h>\n#include<stdlib.h>\n#include<unistd.h>\n#include<fcntl.h>\n#include<sys/mman.h>\n#include<sys/wait.h>\n#include<string.h>\n\nint main(int argc, char const *argv[])\n{\n    char *share;\n    // 创建一个共享内存对象\n    char shm_name[100]  ={0};\n    sprintf(shm_name, \"/letter%d\", getpid());\n    int fd;\n    fd = shm_open(shm_name, O_RDWR | O_CREAT, 0664);\n    if(fd < 0){\n        perror(\"shm_open\");\n        exit(EXIT_FAILURE);\n    }\n\n    // 设置共享内存大小\n    ftruncate(fd, 1024);\n\n    // 内存映射\n    share = mmap(NULL, 1024, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n    if(share == MAP_FAILED){\n        perror(\"mmap\");\n        exit(EXIT_FAILURE);\n    }\n\n    // 映射完成之后关闭fd连接 不是释放\n    close(fd);\n\n    // 使用内存映射实现进程间的通讯\n    pid_t pid = fork();\n    if(pid < 0){\n        perror(\"fork\");\n        exit(EXIT_FAILURE);\n    }\n    if(pid == 0){\n        strcpy(share, \"你是个好人\\n\");\n        printf(\"新学员%d完成回信\\n\", getpid());\n    }else{\n        waitpid(pid, NULL, 0);\n        printf(\"老学员%d收到了新学员%d的回信:%s\", getpid(), pid, share);\n\n        // 释放映射区\n        int re = munmap(share, 1024);\n        if(re == -1){\n            perror(\"munmap\");\n            exit(EXIT_FAILURE);\n        }\n        // 释放共享内存对象\n        shm_unlink(shm_name);\n    }\n\n    return 0;\n}\n  ```\n\n### 消息队列\n• **消息队列的特点**：允许进程通过消息队列发送和接收消息，支持无亲缘关系的进程间通信。\n• **消费者代码示例**：\n  ```c\n#include<mqueue.h>\n#include<stdio.h>\n#include<time.h>\n#include<string.h>\n#include<unistd.h>\n#include<stdlib.h>\n\nint main(int argc, char const *argv[])\n{\n    //创建消息队列\n    struct mq_attr attr;\n\n    //有用的参数 表示消息队列的容量\n    attr.mq_maxmsg = 10;\n    attr.mq_msgsize = 100;\n\n    // 用mq_open时被忽略的参数\n    attr.mq_flags = 0;\n    attr.mq_curmsgs = 0;\n    \n    char *mq_name = \"/p_c_mq\";\n    mqd_t mqdes = mq_open(mq_name, O_RDWR | O_CREAT, 0664, &attr);\n    if(mqdes == (mqd_t)-1){\n        perror(\"mq_open\");\n        exit(EXIT_FAILURE);\n    }\n\n    //不断接收控制台中的数据 发送到消息队列\n    char read_buf[100];\n    struct timespec time_info;\n\n    while(1){\n        memset(read_buf, 0, 100);\n\n        clock_gettime(0, &time_info);\n        time_info.tv_sec += 15;\n        // 读取消息队列的数据\n        if (mq_timedreceive(mqdes, read_buf, 100, NULL, &time_info) == -1)\n        {\n            perror(\"mq_tiemreceive\");\n        }\n\n        //判断当前数据是否为结束信息\n        if(read_buf[0] == EOF){\n            printf(\"接收到生产者发送的结束信息 准备退出...\\n\");\n            break;\n        }\n\n\n        //正常的读取的消息队列信息打印到控制台\n\n        printf(\"接收到来自生产者发送的的消息：%s\\n\", read_buf);\n    }\n\n    //关闭消息队列描述符\n    close(mqdes);\n\n    //清除消息队列\n    mq_unlink(mq_name);\n\n    return 0;\n}\n  ```\n\n• **生产者代码示例**：\n  ```c\n#include<mqueue.h>\n#include<stdio.h>\n#include<time.h>\n#include<string.h>\n#include<unistd.h>\n#include<stdlib.h>\n\nint main(int argc, char const *argv[])\n{\n    //创建消息队列\n    struct mq_attr attr;\n\n    //有用的参数 表示消息队列的容量\n    attr.mq_maxmsg = 10;\n    attr.mq_msgsize = 100;\n\n    // 用mq_open时被忽略的参数\n    attr.mq_flags = 0;\n    attr.mq_curmsgs = 0;\n    \n    char *mq_name = \"/p_c_mq\";\n    mqd_t mqdes = mq_open(mq_name, O_RDWR | O_CREAT, 0664, &attr);\n    if(mqdes == (mqd_t)-1){\n        perror(\"mq_open\");\n        exit(EXIT_FAILURE);\n    }\n\n    //不断接收控制台中的数据 发送到消息队列\n    char write_buf[100];\n    struct timespec time_info;\n\n    while(1){\n        memset(write_buf, 0, 100);\n        ssize_t read_count = read(0, write_buf, 100);\n        clock_gettime(0, &time_info);\n        time_info.tv_sec += 5;\n        if(read_count == -1){\n            perror(\"read\");\n            continue;\n        }else if(read_count == 0){\n            // 就是在控制台中输入 ctrl + d的时候\n            printf(\"EOF, exit..\\n\");\n            char eof = EOF;\n            if (mq_timedsend(mqdes, &eof, 1, 0, &time_info) == -1)\n            {\n                perror(\"mq-timesend\");\n            }\n            break;\n        }\n\n        //正常接收控制台数据并发送\n        if (mq_timedsend(mqdes, write_buf, strlen(write_buf), 0, &time_info) == -1)\n        {\n            perror(\"mq-timesend\");\n        }\n        printf(\"从命令行接收的数据已经发送给消息队列\\n\");\n    }\n\n    //关闭消息队列描述符\n    close(mqdes);\n\n    return 0;\n}\n  ```\n\n### 信号量\n信号量机制由`信号量` 和`P, V操作`两部分组成, `信号量`是一个特殊的变量, 是进程传递的一个整数数值，只能被两个标准原语所访问，及p(wait操作)操作, v(signal操作)操作。P是阻塞原语，V是唤醒原语。P对信号量值-1后, 小于0则执行阻塞操作。反之V +1后， 大于等于零时不执行唤醒操作，小于0则唤醒一个等待进程，继续执行\n\n• **无名信号量**\n通过共享内存的方式实现进程间的通讯\n```c\n#include<pthread.h>\n#include<stdio.h>\n#include<unistd.h>\n#include<fcntl.h>\n#include<semaphore.h>\n#include<sys/mman.h>\n#include<sys/stat.h>\n#include<sys/types.h>\n#include<sys/wait.h>\n\nint main(int argc, char const *argv[])\n{\n    char *shm_name = \"unnamed_sem_shm\";\n    //创建共享内存\n    int fd = shm_open(shm_name, O_CREAT | O_RDWR, 0666);\n    //调整共享内存大小\n    ftruncate(fd, sizeof(sem_t));\n    //完成映射\n    sem_t *sem = mmap(NULL, sizeof(sem_t), PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n    //初始化信号量\n    sem_init(sem,1, 0);\n    //创建父子进程\n    pid_t pid = fork();\n    if(pid < 0){\n        perror(\"fork\");\n    }else if(pid == 0){\n        sleep(1);\n        printf(\"这是子进程\\n\");\n        sem_post(sem);\n    }else{\n        sem_wait(sem);\n        printf(\"这是父进程\\n\");\n        waitpid(pid, NULL, 0);\n    }\n\n    //回收资源\n    if(pid > 0){\n        if(sem_destroy(sem) == -1){\n            perror(\"sem_destroy\");\n        }\n\n    }\n    if(munmap(sem, sizeof(sem_t)) == -1){\n        perror(\"munmap\");\n    }\n    if(close(fd) == -1){\n        perror(\"close\");\n    }\n    if(pid > 0){\n        if(shm_unlink(shm_name) == -1){\n            perror(\"shm_unlink\");\n        }\n    }\n    return 0;\n}\n\n\n```\n\n• **有名信号量**\n比起通过共享内存的方式会简单很多\n```c\n#include<stdio.h>\n#include<unistd.h>\n#include<fcntl.h>\n#include<semaphore.h>\n#include<sys/mman.h>\n#include<sys/stat.h>\n#include<sys/types.h>\n#include<sys/wait.h>\n\nint main(int argc, char const *argv[])\n{\n    //char *shm_value_name = \"/named_sem_shm_value\";\n    char *sem_name = \"/named_sem_shm\";\n\n    //创建共享内存对象\n    // int value_fd = shm_open(sem_name, O_CREAT | O_RDWR, 0666);\n\n    //初始化有名信号量\n    sem_t * sem = sem_open(sem_name, O_CREAT, 0666, 0);\n\n    //创建父子进程\n    pid_t pid = fork();\n    if(pid < 0){\n        perror(\"fork\");\n    }else if(pid == 0){\n        sleep(1);\n        printf(\"这是子进程\\n\");\n        sem_post(sem);\n    }else{\n        sem_wait(sem);\n        printf(\"这是父进程\\n\");\n        waitpid(pid, NULL, 0);\n    }\n\n    //回收资源\n\n    if(sem_close(sem) == -1){\n        perror(\"close\");\n    }\n    if(pid > 0){\n        if(sem_unlink(sem_name) == -1){\n            perror(\"shm_unlink\");\n        }\n    }\n    return 0;\n}\n\n```\n\n• **经典案例**\n通过创建两个信号量来控制进程或线程的执行顺序\n\n```c\n#include<stdio.h>\n#include<unistd.h>\n#include<stdlib.h>\n#include<semaphore.h>\n#include<time.h>\n#include<pthread.h>\n\nsem_t *full;\nsem_t *empty;\n\nint shard_num;\n\nint rand_num(){\n    srand(time(NULL));\n    return rand();\n}\n\nvoid *producer(void *arg){\n    for (size_t i = 0; i < 5; i++)\n    {\n        // 获取信号量\n        sem_wait(empty);\n        printf(\"\\n第%d轮数据传输\\n\", i + 1);\n        sleep(1);\n        shard_num = rand_num();\n\n        //释放信号量\n        sem_post(full);\n    }\n    \n}\n\nvoid *consumer(void *arg){\n    for (size_t i = 0; i < 5; i++)\n    {\n        // 获取信号量\n        sem_wait(full);\n        printf(\"\\n第%ld轮消费者开始读取数据\\n\", i + 1);\n        sleep(1);\n        printf(\"接收到的数据是%d\", shard_num);\n\n        //释放信号量\n        sem_post(empty);\n    }\n    \n}\n\nvoid *consumer(void *arg){\n\n}\n\nint main(int argc, char const *argv[])\n{\n    full = (sem_t *)malloc(sizeof(sem_t));\n    empty = (sem_t *)malloc(sizeof(sem_t));\n    // 初始化信号量\n    sem_init(empty, 0, 1);\n    sem_init(full, 0, 0);\n\n    // 创建生产者和消费者线程\n    pthread_t producer_id, consumer_id;\n    pthread_create(&producer_id, NULL, producer, NULL);\n    pthread_create(&consumer_id, NULL, consumer, NULL);\n\n    //等待线程全部完成\n    pthread_join(producer_id, NULL);\n    pthread_join(consumer_id, NULL);\n\n    //代码最后摧毁信号量\n    sem_destroy(full);\n    sem_destroy(empty);\n    return 0;\n}\n\n```\n\n### 信号\n• **信号的特点**：用于通知进程发生了某种事件，是一种异步通信方式。\n• **代码示例**：\n  ```c\n#include<stdio.h>\n#include<stdlib.h>\n#include<signal.h>\n#include<unistd.h>\n\nvoid sigint_handle(int signum){\n    printf(\"\\n受到%d信号 停止程序\\n\", signum);\n    exit(signum);\n}\n\nint main(int argc, char const *argv[])\n{\n    if (signal(SIGINT, sigint_handle) == SIG_ERR)\n    {\n        perror(\"signal\");\n        return 1;\n    }\n    \n    while(1){\n        sleep(1);\n        printf(\"你好, 在吗\\n\");\n    }\n    return 0;\n}\n  ```\n\n---","tags":["Linux","C"],"categories":["Linux应用层"]},{"title":"1v1视频聊天网页","url":"/2025/01/19/1v1视频聊天网页/","content":"\n花了点时间跟着B站的教程手敲了一个关于WebRtc的项目，它是B/S框架的，用js做的前后端。工具用的是node，可以说是非常的方便。js语法本身也简单易学，和C有很多相似之处。总的来说nodejs YYDS(q(≧▽≦q))\n\n\n### 整个项目可以分为三大块\n\n1. 身份验证:登录、注册\n\n涉及到前端界面，数据库，信令服务器，node web服务器\n\n2. 聊天室\n\n涉及到前端界面，信令服务器，node web服务器\n\n3. 1v1聊天室\n\n涉及到前端界面，信令服务器，node web服务器，NAT穿透服务器(当然我这里没有搭建，所以候选者用的host，毕竟就是为了学习WebRtc)\n\n### 相关设计\n\n1、B/S架构模式\n\n2、登录界面设计\n\n3、音视频聊天界面设计\n\n4、文字输入输出界面设计\n\n5、房间创建与退出功能\n\n6、音视频聊天服务\n\n7、文字聊天服务处理\n\n8、信令服务处理\n\n9、NAT穿透服务处理\n\n### 用到的node模块\n\nhttp https 用于支持http https协议\nsqlite3 数据库模块\nlog4js 主要用于服务器的控制台输出日志\nsocket.io 用于信令服务器的设计（额外还需要再客户端引入对应版本的socket.io.js）\nexpress server-index 两者结合使用，用于将网页以文件夹的形式展示出来\n\n---\n内容很多，以下主要展示信号监听相应的js代码，项目的主要内容也就是这里\n\n### server.js（服务器）\n```js\n\n//引入的模块\nvar http=require(\"http\");\nvar https = require(\"https\");\nvar fs = require(\"fs\");\n\n    //自己安装的\nvar express=require(\"express\");\nvar serveIndex=require(\"serve-index\");\nvar sqlite3=require(\"sqlite3\");\nvar log4js=require(\"log4js\");\nvar socket=require(\"socket.io\");\n\n......\n\nvar io=socket.listen(httpServer);\nio.sockets.on(\"connection\", (socket)=>{\n    logger.info(\"connected\", socket.id);\n\n    //开始监听身份验证相关的信号\n    socket.on(\"login\", (uname, pwd)=>{\n        logger.info(\"login: \", uname, pwd);\n        db.all(\"select * from users where name=? and pwd=?\",\n            [uname, pwd], (e, rows)=>{\n            if(e){\n                handleErr(e);\n                socket.emit(\"serverErr\");\n            }else{\n                if(rows.length===1){\n                    socket.emit(\"loginsuc\", uname);\n                }else{\n                    socket.emit(\"loginfailed\", uname);\n                }\n            }\n        })\n    });\n\n    socket.on(\"regist\", (uname, pwd)=>{\n        var sql = \"select * from users where name=?\";\n        db.all(sql, [uname], (e, rows)=>{\n           if(e){\n               handleErr(e);\n               socket.emit(\"serverErr\");\n           }else{\n               if(rows.length===1){\n                   socket.emit(\"samename\", uname);\n               }else{\n                   sql=\"insert into users(name, pwd) values(?, ?)\";\n                   db.run(sql, [uname, pwd], (e)=>{\n                       if(e){\n                           handleErr(e);\n                           socket.emit(\"serverErr\");\n                       }else{\n                           socket.emit(\"registsuc\");\n                       }\n                   })\n               }\n           }\n        });\n    })\n\n    //监听聊天大厅相关的信号\n    socket.on(\"cjoin\", (room, uname)=>{\n        logger.info(\"cjoin\", room, uname);\n        socket.join(room);\n\n        socket.emit(\"cjoined\", uname);\n        socket.to(room).emit(\"cotherjoined\", uname);\n    })\n\n    socket.on(\"cmessage\", (room, uname, msg)=>{\n        logger.info(\"cmessage\", room, uname, msg);\n        io.in(room).emit(\"cgetmessage\", uname, msg);\n    })\n\n    socket.on(\"cleave\", (room, uname)=>{\n        logger.info(\"cleave\", room, uname);\n        socket.leave(room);\n        socket.emit(\"cleft\");\n        socket.to(room).emit(\"cotherleft\", uname);\n    })\n\n    //开始处理1v1聊天室的消息\n    socket.on(\"vjoin\", (room, uname)=>{\n        logger.info(\"vjoin\", room, uname);\n        socket.join(room);\n\n        var myRoom = io.sockets.adapter.rooms[room];\n        var users = Object.keys(myRoom.sockets).length;\n        logger.info(room+\" users=\" + users);\n        if(users > 2){\n            socket.leave(room);\n            socket.emit(\"vfull\", room);\n        }else{\n            socket.emit(\"vjoined\", room);\n            if(users > 1){\n                socket.to(room).emit(\"votherjoined\", room, uname);\n            }\n        }\n    });\n\n    socket.on(\"vdata\", (room, data)=>{\n        logger.info(\"vdata\", room, data);\n        socket.to(room).emit(\"vgetdata\", room, data);\n    });\n\n    socket.on(\"vleave\", (room, uname)=>{\n        logger.info(\"vleave\", room, uname);\n        if(room === \"\" || uname === \"\"){\n            logger.info(\"room is empty string\");\n            return;\n        }\n        var myRoom = io.sockets.adapter.rooms[room];\n        var users = Object.keys(myRoom.sockets).length;\n\n        logger.info(\"vleave users=\" + (users - 1));\n        socket.leave(room);\n        socket.emit(\"vleft\", room);\n        socket.to(room).emit(\"votherleft\");\n    })\n\n});\n```\n\n### login.js（登录）\n\n```js\nfunction start(){\n    socket=io.connect();\n\n    //监听服务器的消息\n    socket.on(\"loginsuc\",(uname)=>{\n        alert(\"登录成功:\" + uname);\n        window.location.href=\"chat.html?uname=\"+uname;\n    });\n\n    socket.on(\"loginfailed\",(uname)=>{\n        alert(\"登录失败:\" + uname);\n    });\n\n    socket.on(\"serverErr\",()=>{\n         alert(\"服务器操作异常\");\n    })\n}\n```\n\n### regist.js（注册）\n\n```js\nfunction start(){\n    socket=io.connect();\n\n    socket.on(\"serverErr\", ()=>{\n        alert(\"服务器操作异常，请重试\");\n    });\n\n    socket.on(\"samename\", (uname)=>{\n        alert(\"改名字已被注册\");\n    });\n\n    socket.on(\"registsuc\", ()=>{\n        alert(\"注册成功\");\n        goBack();\n    });\n}\n```\n\n### chat.js（聊天室）\n\n这里的taMsgList指向的是一个textarea标签\n\n```js\nfunction start(){\n    socket=io.connect();\n\n    socket.on(\"cjoined\", (uname)=>{\n        console.log(taMsgList); // 检查taMsgList是否正确指向textarea\n        taMsgList.value = \"欢迎\" + uname + \"来到大厅\\n\";\n    });\n\n    socket.on(\"cotherjoined\", (uname)=>{\n        taMsgList.value += uname + \"进入大厅\\n\";\n    })\n\n    socket.on(\"cgetmessage\", (uname, msg)=>{\n        taMsgList.value += uname + \":\" + msg + \"\\n\";\n    })\n\n    socket.on(\"cleft\", ()=>{\n        history.back();\n    })\n\n    socket.on(\"cotherleft\", (uname)=>{\n        taMsgList.value += uname + \"离开了大厅\\n\";\n    })\n\n    socket.emit(\"cjoin\", room, uname);\n}\n```\n\n### videoRoom.js(视频聊天)\n\n这里有一个状态机的概念，基status的状态，它的设计是为保证所有情况的发生都有正确的处理\n```js\nfunction conn(){\n    socket = io.connect();\n    //监听来自服务器的信号\n    socket.on(\"vfull\", (room)=>{\n        status = \"leave\";\n        alert(\"房间已满: \" + room);\n        console.log(\"vfull:\", status);\n    });\n\n    socket.on(\"vjoined\", (room)=>{\n        alert(\"成功加入房间:\" + room);\n\n        iptRoom.disabled = true;\n        btnEnterRoom.disabled = true;\n        btnLeaveRoom.disabled = false;\n\n        createPeerConnection();\n\n        status = \"joined\";\n        console.log(\"vjoined:\", status);\n    });\n\n    socket.on(\"votherjoined\", (room, uname)=>{\n        console.log(\"别人进来了了:\"+uname);\n\n        if(status === \"joined_unbind\"){\n            createPeerConnection();\n        }\n        status = \"joined_conn\";\n\n        //媒体协商\n        mediaNegociate()\n\n        console.log(\"votherjoined:\", status);\n    });\n\n    socket.on(\"vgetdata\", (room, data)=>{\n       console.log(\"vgetdata:\", data);\n       if(!data){\n           return;\n       }\n       if(data.type===\"candidate\"){\n           console.log(\"get other candidate\");\n           //候选者信息\n           var cddt = new RTCIceCandidate({\n               sdpMLineIndex:data.label,\n               candidate:data.candidate\n           });\n           pc.addIceCandidate(cddt);\n       }else if(data.type === \"offer\"){\n           console.log(\"get offer\");\n           pc.setRemoteDescription(new RTCSessionDescription(data));\n\n           //查询自己的媒体信息并应答\n           pc.createAnswer()\n               .then(getAnswer)\n               .catch(handleErr);\n       }else if(data.type === \"answer\"){\n           console.log(\"get answer\");\n           pc.setRemoteDescription(new RTCSessionDescription(data));\n       }\n    });\n\n    socket.on(\"vleft\", (room)=>{\n        alert(\"离开房间:\"+room);\n\n        iptRoom.disabled = false;\n        btnEnterRoom.disabled = false;\n        btnLeaveRoom.disabled = true;\n\n        status = \"leaved\";\n        console.log(\"vleft:\", status);\n    });\n\n    socket.on(\"votherleft\", ()=>{\n        status = \"vjoined_unbind\";\n        closePeerConnection();\n        console.log(\"votherleft:\", status);\n    });\n}\n```","tags":["WebRtc"],"categories":["音视频"]},{"title":"WebRtc初步认识(webrtc实现1v1视频通话从0到1)","url":"/2024/12/25/WebRtc初步认识(webrtc实现1v1视频通话从0到1)/","content":"\n在网上找到了一个非常适合入门的教程，分享给有缘人(稍微改了以下布局，并添加了一点内容o(*￣︶￣*)o)\n\n## WebRtc处理过程\n\n实现1v1的通话有4个部分，WebRtc终端（这里理解为浏览器端）、Signal（信令）服务器、STUN/TURN服务器\n\n- WebRtc终端，负责音视频的采集、编解码、NAT穿越、音视频数据传输 （这里终端暂时看做浏览器，webRtc不止应用在浏览器）\n- Signal服务器，负责信令处理，如有人加入房间、离开房间、媒体协商消息传递等。（类似聊天室的xx加入房间），一般采用WebSocket连接\n- STUN/TURN服务，负责获取WebRtc终端在公网的ip地址，以及NAT穿越失败后的数据中转。（）\n\n用户A和用户B要语音通过大致过程：\n1. 用户A和用户B作为WebRtc终端（浏览器）检测你的设备是否支持音视频数据采集，\n2. 获取音视频数据后加入到信令信令服务器，这样2个用户都加入到一个房间\n3. 用户A会创建RTCPeerConnection对象，该对象将采集到的音视频数据进行编码和通过P2P传送给对方，P2P穿越失败，就使用TURN进行数据中转，有的公司架构是直接用后者进行传输\n音视频采集\n浏览器的getUserMedia方法\n\n```js\nnavigator.mediaDevices.getUserMedia(constraints);\n```\n\nconstraints参数视频设置采集分辨率、帧率参数，音频可以设置开启降噪等参数；如设备具备音视频采集能力，它返回的成功的promise里，可以获取到MediaStream对象，并完成以下操作\n1. 本地操作视频流：MediaStream对象存放着采集到的音视频轨，直接赋值给video标签的srcObject属性，就可以本地实现看到摄像头和听到声音。\n2. 拍照：通过canvas的drawImage，将video标签传入\n3. 保存照片：通过canvas.toDataURL生成本地地址，通过a标签下载图片\n\n```js\nvar constraints = { audio: true, video: true };\nlet media\nnavigator.mediaDevices\n    .getUserMedia(constraints)\n    .then((MediaStream) => {\n      media = MediaStream;\n      const video = document.querySelector(\"video\");\n      video.srcObject = MediaStream;\n      video.onloadedmetadata = function (e) {\n        video.play();\n      };\n    })\n    .catch((e) => {\n      console.warn(e, \"e\");\n    });\n\n// 拍照：调用canvas的api，将video标签传入\nconst video = document.querySelector(\"video\");\ndocument.querySelector(\"canvas\").getContext('2d').drawImage(video, 0, 0,400,300)\n\n// 保存照片\nconst url = canvas.toDataURL(\"image/jpeg\");\ndocument.createElement('a').href = url;\n```\n\n### 媒体协商\n\n- **作用**：  \n  让双方找到共同支持的媒体能力，过程有点像 TCP 的三次握手。\n\n- **知识点**：\n  1. **创建连接**：  \n     创建 `RTCPeerConnection` 对象，它负责端与端之间建立 P2P 连接。\n  2. **信令**：  \n     客户端通过信令服务器交换 **SDP**（Session Description Protocol）信息。SDP 包含编解码方式、传输协议、IP 地址和端口等信息，确保双方能够找到共同支持的媒体能力并进行通信。\n  3. **Offer**：  \n     在双向通信时，呼叫方发送的 SDP 消息称为 **Offer**。\n  4. **Answer**：  \n     在双向通信时，被呼叫方发送的 SDP 消息称为 **Answer**。\n\n- **媒体协商过程**：\n  1. 呼叫方创建 Offer 类型的 SDP 消息后，通过 `setLocalDescription` 方法保存到本地的 Local 域，再通过信令将 Offer 发给被呼叫方。\n  2. 被呼叫方收到 Offer 类型的 SDP 消息后，通过 `setRemoteDescription` 保存到其 Remote 域。接着，它创建 Answer 类型的 SDP 消息，并通过 `setLocalDescription` 保存到本地，再将消息发给呼叫方。\n  3. 呼叫方收到 Answer 类型的 SDP 消息后，通过 `setRemoteDescription` 保存到其 Remote 域。\n\n- **总结**：  \n  媒体协商完成后，WebRTC 底层会收集 **Candidate**（WebRTC 与远端通信时使用的协议、IP 地址和端口），进行连通性测试，最终建立一条链路。\n\n\n```js\n// 呼叫方A\n// 创建Offer,本地设置，发给对方\nconst pcA = new RTCPeerConnection();\npcA.createOffer().then((offerSDP)=>{\n    pcA.setLocalDescription(offerSDP);\n    sendMessage(offerSDP);\n})\n\n// 接受到answer，保存起来\nsocket.on('message', (message) => {\n    if(message.type === 'answer') {\n        pcA.setRemoteDescription(new RTCSessionDescription(message))\n    }\n})\n\n\n// 被呼叫方B\nconst pcB = new RTCPeerConnection();\nsocket.on('message', (message) => {\n    if(message.type === 'offer')\n    pcB.setRemoteDescription(new RTCSessionDescription(message))\n    \n    // 创建Answer,本地设置，发给对方\n    pcB.createAnswer().then((answerSDP)=>{\n        pcB.setLocalDescription(answerSDP);\n        sendMessage(answerSDP)\n    })\n})\n\n```\n\n### 连接建立\n\n#### 连接的基本原则\n如果A和B连接，C作为服务器\n\n场景一：双方处于同一个网段内（内网）\n场景二：双方处于不同点\n\n`ICE Candidate（ICE候选者）`，它表示WebRtc与远端通信使用的协议、ip地址和端口，一般3种方式\n`host`表示本机候选者，内网之间的联通性测试，优先级最高\n`srflx`表示内网主机映射的外网地址和端口，让双发通过P2P进行连接，次优化级\n`relay`表示中继候选者，低优先级\n\n```js\n{\n    address: 'xxx.xxx.xxx.xxx',\n    port: 'xxxx',\n    type: 'host/srflx/relay',\n    protocol: 'UDP/TCP',\n    // ...\n}\n```\n\n#### STUN协议\n`NAT`是指网络地址转换，作用就是进行内外网的地址转换。\n`STUN(session traversal utilities for NAT)`,一种处理NAT传输的协议，它允许位于NAT后的客户端找出自己的公网地址。\nsrflx类型的Candidate实际上就是用的经NAT映射后的外网地址，进行P2P连接通信。\n#### TURN协议\nrelay服务通过TURN协议实现。TURN协议描述了如何获取relay服务器（即TUNR协议）的Candidate过程。通过TURN服务器发送Allocation指令，relay服务就会在服务端分配一个新的relay端口，用于中转UDP数据报。\n因为P2P场景有限，其实大部分还是采用relay方式来传输数据。\n实现1v1音视频实时直播系统\n用户A和用户B之间视频通话过程：\n\n#### 信令服务器\n基于nodejs\n```js\nconst ws = require('nodejs-websocket');\nconst roomTableMap = new Map(); // 根据roomId，记录每个房间的成员信息\nws.createServer((socket) => {\n    socket.on('text', (str) => {\n        // 接收来自客户端的消息\n    })\n\n    socket.on('close', (data) => {\n        console.log(data);\n    })\n\n    socket.on('error', (data) => {\n        console.log(data);\n    })\n}).listen(3000)\n```\n\n#### 建立连接的具体流程\n1. 用户A和用户B，通过WebSocket，连接ws服务，监听回调函数\n```js\nfunction createWebScoket(url) {\n  ws = new WebSocket(url);\n  ws.onopen = (e) => {};\n  ws.onmessage = (e) => {\n    // 接收来自信令服务器的消息\n  };\n  ws.onclose = (e) => {};\n  ws.onerror = (e) => {};\n}\n```\n\n2. 用户A和用户B先后点击加入房间，根据navigator.mediaDevices.getUserMedia获取本地的音视频流，再通过ws.send将roomId、userId等信息传递给信令服务器\n```js\nroomId = document.getElementById('roomBox').value // 输入框取房间号\n// 拿到媒体设备流，显示在本地\nnavigator.mediaDevices.getUserMedia({ video: true, audio: true }).then((stream) => {\n    ws.send(JSON.stringify({\n      type: 'join',\n      roomId,\n      uid: localUserId\n    }))\n    localStream = stream;\n    localVideo.srcObject = stream;\n})\n```\n\n3. 信令服务器先后收到用户A和用户B消息，将该用户加入房间，当用户B加入的时候，就通知到房间里用户A\n```js\nsocket.on('text', (data) => {\n    const { type = '', roomId, uid, remoteUid} = JSON.parse(data);\n    if (type === 'join') {\n        let roomMap = roomTableMap.get(roomId);\n        if (!roomMap) {\n            roomMap = new Map();\n            roomTableMap.set(roomId, roomMap)\n        }\n        roomMap.set(uid, {roomId,uid,socket})\n        for (const [key, value] of roomMap) {\n            if(key !== uid) {\n                value.socket.sendText(JSON.stringify({ type: 'peer-join', remoteUid: uid }));\n            }\n        }\n    }\n})\n```\n\n\n4. 用户A发现此时房间有其他人，会收到消息，type=“peer-join”，会先创建RTCPeerConnection对象，再监听onicecandidate（收到网络协商相关消息）和ontrack（收到远端音视频流）,将本地音视频流添加到对象中\n```js\nws.onmessage = (e) => {\n    if (type === 'peer-join') {\n        pc = new RTCPeerConnection(null);\n        pc.onicecandidate = (e) => {\n            if (e.candidate) {\n              ws.send(JSON.stringify({\n                type: 'candidate',\n                roomId,\n                uid: localUserId,\n                remoteUid: remoteUserId,\n                candidate: e.candidate\n              }))\n            }\n        }\n        pc.ontrack = (e) => {\n            remoteStream = e.streams[0];\n            remoteVideo.srcObject = remoteStream;\n        }\n        localStream.getTracks().forEach(track => {\n        pc.addTrack(track, localStream)\n        });\n    }\n}\n```\n\n\n5. 用户A再通过RTCPeerConnection的实例，创建offer（sdp信息用于协商音视频编码协议），setLocalDescription设置在本地，再发送到信令服务器,转给用户B\n```js\nws.onmessage = (e) => {\n    if (type === 'peer-join') {\n      pc.createOffer().then(sdp => {\n          pc.setLocalDescription(sdp).then(() => {\n              ws.send(JSON.stringify({\n                type: 'offer',\n                roomId,\n                uid: localUserId,\n                remoteUid,\n                sdp\n              }))\n            })\n        })\n    }\n}\n```\n\n6. 信令服务器收到消息，type=‘offer’时，转发给用户B\n```js\nsocket.on('text', (data) => {\n    const { type = '', roomId, uid, sdp = '' } = JSON.parse(str);\n    if(type === 'offer') {\n        let roomMap = roomTableMap.get(roomId);\n        for (const [key, value] of roomMap) {\n            if (key !== uid) {\n                value.socket.sendText(JSON.stringify({\n                  type,\n                  roomId,\n                  remoteUid: uid,\n                  sdp\n                }))\n              }\n        }\n    }\n})\n```\n\n\n7. 用户B，收到信令服务器发到type=‘offer’消息，同样先创建RTCPeerConnection实例和监听，再监听onicecandidate和ontrack，将本地音视频流添加到实例中，代码同上面一致\n同时用户B，会将用户A的sdp消息，setRemoteDescription到远程，自己创建answer的sdp信息，发送给服务端，转给用户A\n```js\nws.onmessage = (e) => {\n    if (type === 'offer') {\n        pc.setRemoteDescription(new RTCSessionDescription(sdp));\n        pc.createAnswer().then(sdp => {\n            pc.setLocalDescription(sdp).then(() => {\n              ws.send(JSON.stringify({\n                type: 'answer',\n                roomId,\n                uid: localUserId,\n                remoteUid,\n                sdp\n              }))\n            })\n        })\n    }\n}\n```\n\n\n8. 最后用户A收到，type=‘answer’的消息，setRemoteDescription到远程\n双方收到ontrack回调事件，获取到对方码流的对象句柄\n双方都开始请求打洞，通过onicecandidate获取到打洞信息（candidate）并通过信令服务器发送给对方\n如果p2p能成功则进行通话，不成功则进行中继转发通话(这里并没有配置服务器，所以只能进行内网的通信)\n\n#### 中继服务器的配置\n\n这里我使用了开源的coturn服务器\n\n添加pc = new RTCPeerConnection()这个方法的参数\n```js\nconst configuration = {\n  iceTransportPolicy: 'relay', //测试的时候使用relay, 发布到公网时用all\n  iceServers: [\n    {\n      urls:  ['stun:192.168.75.128:3478', 'stun:stun.l.google.com:19302'] //可以按照这样的格式填写多个\n    },\n    {\n      urls: 'turn:192.168.75.128:3478', //你的服务器\n      username: 'wqe', //用户名\n      credential: '1234567' //密码\n    }\n  ]\n};\n```\n\n### 完整代码\n#### 前端代码\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Document</title>\n</head>\n<body>\n  <div>\n    <input id=\"roomBox\" type=\"text\">\n    <button id=\"joinBtn\">加入</button>\n    <button id=\"leaveBtn\">离开</button>\n  </div>\n  <div>\n    <video id=\"localVideo\" autoplay muted playsinline></video>\n    <video id=\"remoteVideo\" autoplay playsinline></video>\n  </div>\n  <script>\n    const localVideo = document.getElementById('localVideo');\n    const remoteVideo = document.getElementById('remoteVideo');\n    const joinBtn = document.getElementById('joinBtn');\n    const leaveBtn = document.getElementById('leaveBtn');\n    const localUserId = Math.random().toString(36).slice(2);\n    let roomId = -1;\n    let remoteUserId = -1;\n    let localStream = null;\n    let remoteStream = null;\n    let ws = null;\n    let pc = null;\n    function createWebScoket(url) {\n      ws = new WebSocket(url);\n      ws.onopen = (e) => {\n        console.log(\"onopen\", e)\n      }\n      ws.onmessage = (e) => {\n        const { type, remoteUid = '', sdp, candidate } = JSON.parse(e.data);\n        if (type === 'peer-join') {\n          remoteUserId = remoteUid;\n          if (!pc) createConnect();\n          pc.createOffer().then(sdp => {\n            pc.setLocalDescription(sdp).then(() => {\n              ws.send(JSON.stringify({\n                type: 'offer',\n                roomId,\n                uid: localUserId,\n                remoteUid,\n                sdp\n              }))\n            })\n          })\n        } else if (type === 'peer-leave') {\n          remoteVideo.srcObject = null;\n        } else if (type === 'offer') {\n          if (!pc) createConnect();\n          pc.setRemoteDescription(new RTCSessionDescription(sdp))\n          pc.createAnswer().then(sdp => {\n            pc.setLocalDescription(sdp).then(() => {\n              ws.send(JSON.stringify({\n                type: 'answer',\n                roomId,\n                uid: localUserId,\n                remoteUid,\n                sdp\n              }))\n            })\n          })\n        } else if (type === 'answer') {\n          if (!pc) createConnect();\n          pc.setRemoteDescription(new RTCSessionDescription(sdp))\n        } else if (type === 'candidate') {\n          pc.addIceCandidate(new RTCIceCandidate(candidate))\n        }\n      }\n      ws.onclose = (e) => {\n        console.log(\"onclose\", e)\n      }\n      ws.onerror = (e) => {\n        console.log(\"onerror\", e)\n      }\n    }\n    createWebScoket('ws://127.0.0.1:3000');\n    // 加入房间\n    joinBtn.onclick = () => {\n      roomId = document.getElementById('roomBox').value // 输入框取房间号\n      // 拿到媒体设备流，显示在本地\n      navigator.mediaDevices.getUserMedia({ video: true, audio: true }).then((stream) => {\n        ws.send(JSON.stringify({\n          type: 'join',\n          roomId,\n          uid: localUserId\n        }))\n        localStream = stream;\n        localVideo.srcObject = stream;\n      })\n    }\n    function createConnect() {\n      pc = new RTCPeerConnection(null);\n      pc.onicecandidate = (e) => {\n        if (e.candidate) {\n          ws.send(JSON.stringify({\n            type: 'candidate',\n            roomId,\n            uid: localUserId,\n            remoteUid: remoteUserId,\n            candidate: e.candidate\n          }))\n        }\n      }\n      pc.ontrack = (e) => {\n        remoteStream = e.streams[0];\n        remoteVideo.srcObject = remoteStream;\n      }\n      localStream.getTracks().forEach(track => {\n        pc.addTrack(track, localStream)\n      });\n    }\n    // 离开\n    leaveBtn.onclick = () => {\n      ws.send(JSON.stringify({\n        type: 'leave',\n        roomId,\n        uid: localUserId\n      }))\n      remoteVideo.srcObject = null;\n      localStream && localStream.getTracks().forEach(track => track.stop())\n      localVideo.srcObject = null;\n      if(pc) {\n        pc.close();\n        pc = null;\n      }\n    }\n  </script>\n</body>\n</html>\n```\n\n#### 信令服务器完整代码\n```js\nconst ws = require('nodejs-websocket');\nconst roomTableMap = new Map();\nws.createServer((socket) => {\n    socket.on('text', (str) => {\n        const { type = '', roomId, uid, remoteUid, candidate = '', sdp = '' } = JSON.parse(str);\n        if (type === 'join') {\n            let roomMap = roomTableMap.get(roomId);\n            if (!roomMap) {\n                roomMap = new Map();\n                roomTableMap.set(roomId, roomMap)\n            }\n            const client = {\n                roomId,\n                uid,\n                socket\n            }\n            roomMap.set(uid, client)\n            for (const [key, value] of roomMap) {\n                if (key !== uid) {\n                    // 通知其他人，有人进入房间了\n                    value.socket.sendText(JSON.stringify({ type: 'peer-join', remoteUid: uid }));\n                }\n            }\n        } else if (type === 'leave') {\n            let roomMap = roomTableMap.get(roomId);\n            roomMap.delete(uid);\n            for (const [key, value] of roomMap) {\n                value.socket.sendText(JSON.stringify({ type: 'peer-leave', remoteUid: uid }));\n            }\n        } else if (['offer', 'answer', 'candidate'].includes(type)) {\n            let roomMap = roomTableMap.get(roomId);\n            for (const [key, value] of roomMap) {\n              if (key !== uid) {\n                value.socket.sendText(JSON.stringify({\n                  type,\n                  roomId,\n                  remoteUid: uid,\n                  sdp,\n                  candidate\n                }))\n              }\n            }\n        }\n    })\n    socket.on('close', (data) => {\n        console.log(data);\n    })\n    socket.on('error', (data) => {\n        console.log(data);\n    })\n}).listen(3000)\n```\n\n### 最后\n本文只是webrtc进行一个初步的了解，参考了<<WebRtc音视频实时互动技术原理>>书籍和b站视频，对于大家如果有所帮助，欢迎点赞~\n\n作者：竹业\n链接：https://juejin.cn/post/7218532789195702333\n来源：稀土掘金\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","tags":["WebRtc"],"categories":["音视频"]},{"title":"FFmpeg与SDL实现真正的音视频播放器","url":"/2024/12/18/FFmpeg与SDL实现真正的音视频播放器/","content":"通过学习跟着写了一个音视频播放器。内容有点多，所以我就不打算在博客都展示出来了，这里就展示一下每个类的核心逻辑。思路很清晰，这里就简单讲解一下吧((～o￣3￣)～)\n#### 音视频队列\n- packet队列\n`avpacketqueue` 存储解复用线程中读取到的packet数据，因为队列的实现比较简单，我就展示一下该类的结构吧\n\n```cpp\nclass AVPacketQueue\n{\npublic:\n    AVPacketQueue();\n    ~AVPacketQueue();\n    void Abort();\n    int Size();\n    int Push(AVPacket *val);\n    AVPacket *Pop(const int timeout);\nprivate:\n    void Release();\n    Queue<AVPacket *>queue_;\n};\n```\n\n- frame视频队列\n`avframequeue` 存储解码线程中解码后的frame数据，类结构如下\n```cpp\nclass AVFrameQueue\n{\npublic:\n    AVFrameQueue();\n    ~AVFrameQueue();\n    void Abort();\n    int Push(AVFrame *val);\n    AVFrame *Pop(const int timeout);\n    AVFrame *Front();\n    int Size();\n\nprivate:\n    void release();\n    Queue<AVFrame *>queue_;\n};\n```\n\n#### 线程分工\n- 主线程\n主要完成线程的创建，让子线程完成具体的工作。因为主线程对程序的理解挺重要的，所以你可以在文章的最后直接看到主线程的源码\n\n- 解复用线程\n`demuxthread` 完成音频的解复用，并将pkt数据写入packet队列，同时音视频的相关参数也由该线程提供。\n```cpp\nwhile(abort_ != 1){\n    ret = av_read_frame(ifmt_ctx_, &pkt);\n    if(ret < 0){\n        av_strerror(ret, err2str, sizeof(err2str));\n        av_log(NULL, AV_LOG_ERROR, \"av_read_frame(ifmt_ctx_, &pkt) failed, ret: %s\\n\", err2str);\n        break;\n    }\n    if(pkt.stream_index == audio_index_){\n        audio_queue_->Push(&pkt);\n        av_log(NULL, AV_LOG_INFO, \"audio pkt queue size:%d\\n\", audio_queue_->Size());\n    }else if(pkt.stream_index == video_index_){\n        video_queue_->Push(&pkt);\n        av_log(NULL, AV_LOG_INFO, \"video pkt queue size:%d\\n\", video_queue_->Size());\n    }else {\n        av_packet_unref(&pkt);\n    }\n}\n```\n\n- 解码线程\n`decodethread` 完成音视频的解码，并将解码后的数据写入frame队列。\n\n```cpp\nwhile(abort_ != 1){\n    // if(frame_queue_->Size() > 10){\n    //     std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    //     continue;\n    // }\n    AVPacket *pkt = packet_queue_->Pop(10);\n    if(pkt){\n        int ret = avcodec_send_packet(codec_ctx_, pkt);\n        av_packet_free(&pkt);\n        if(ret < 0){\n            av_strerror(ret, err2str, sizeof(err2str));\n            av_log(NULL, AV_LOG_ERROR, \"avcodec_send_packet(codec_ctx_, pkt) failed, ret: %s\\n\", err2str);\n            break;\n        }\n        // 读取解码后的数据frame\n        while(true){\n            ret = avcodec_receive_frame(codec_ctx_, frame);\n            if(ret == 0){\n                frame_queue_->Push(frame);\n                av_log(NULL, AV_LOG_INFO, \"%s frame queue size:%d\\n\",codec_ctx_->codec->name, frame_queue_->Size());\n                continue;\n            }else if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                break;\n            }else {\n                abort_ = 1;\n                av_strerror(ret, err2str, sizeof(err2str));\n                av_log(NULL, AV_LOG_ERROR, \"avcodec_receive_frame(codec_ctx_, frame) failed, ret: %s\\n\", err2str);\n                break;\n            }\n        }\n    }else {\n        //av_log(NULL, AV_LOG_INFO, \"not get packet\\n\");\n    }\n}\n```\n\n#### 输出渲染\n\n- 音频输出\n`audiooutput` 这里涉及到SDL对播放音频的处理。实际上SDL会自己建立线程完成对音频数据的获取，即通过回调的方式，所以最主要的就是要设计好回调函数，下面是回调函数的设计\n\n```cpp\nvoid fill_audio_pcm(void *udata, Uint8 *stream, int len){\n    //1. 从frame queue读取解码后的PCM数据，填充到stream\n    AudioOutput *is = (AudioOutput *)udata;\n    int len1 = 0;\n    int audio_size = 0;\n    while(len > 0){\n        if(is->audio_buf_index == is->audio_buf_size){\n            is->audio_buf_index = 0;\n            AVFrame *frame = is->frame_queue_->Pop(10);\n            if(frame){\n                is->pts_ = frame->pts;\n                //判断要不要重采样\n                if((frame->format != is->dst_tgt_.fmt)\n                    || (frame->sample_rate != is->dst_tgt_.freq)\n                    || (frame->ch_layout.nb_channels != is->dst_tgt_.channle_layout.nb_channels)\n                    && (!is->swr_ctx_)){\n                    int ret = swr_alloc_set_opts2(&is->swr_ctx_,\n                                        &is->dst_tgt_.channle_layout,\n                                        (enum AVSampleFormat)is->dst_tgt_.fmt,\n                                        is->dst_tgt_.freq,\n                                        &frame->ch_layout,\n                                        (enum AVSampleFormat)frame->format,\n                                        frame->sample_rate,\n                                        0, NULL);\n                    if(ret != 0 || swr_init(is->swr_ctx_) < 0){\n                        av_log(NULL, AV_LOG_ERROR, \"swr_alloc_set_opts2 or swr_init(is->swr_ctx_) failed!\\n\");\n                        swr_free(&is->swr_ctx_);\n                        return ;\n                    }\n                }\n                if(is->swr_ctx_){\n                    const uint8_t **in = (const uint8_t **)frame->extended_data;\n                    uint8_t **out = &is->audio_buf1_;\n                    int out_samples = frame->nb_samples * is->dst_tgt_.freq / frame->sample_rate + 256;\n                    int out_bytes = av_samples_get_buffer_size(NULL, is->dst_tgt_.channles, out_samples, is->dst_tgt_.fmt, 0);\n                    if(out_bytes < 0){\n                        av_log(NULL, AV_LOG_ERROR, \"av_samples_get_buffer_size failed!\\n\");\n                        return ;\n                    }\n                    av_fast_malloc(&is->audio_buf1_, &is->audio_buf1_size, out_bytes);\n                    int len2 = swr_convert(is->swr_ctx_, out, out_samples, in, frame->nb_samples); //返回样本数\n                    if(len2 < 0){\n                        av_log(NULL, AV_LOG_ERROR, \"swr_convert failed!\\n\");\n                        return ;\n                    }\n                    is->audio_buf_ = is->audio_buf1_;\n                    is->audio_buf_size = av_samples_get_buffer_size(NULL, is->dst_tgt_.channles, len2, is->dst_tgt_.fmt, 1);\n                }else {\n                    audio_size = av_samples_get_buffer_size(NULL, frame->ch_layout.nb_channels, frame->nb_samples, (enum AVSampleFormat)frame->format, 1);\n                    av_fast_malloc(&is->audio_buf1_, &is->audio_buf1_size, audio_size);\n                    is->audio_buf_ = is->audio_buf1_;\n                    is->audio_buf_size = audio_size;\n                    memcpy(is->audio_buf_, frame->data[0], audio_size);\n                }\n                av_frame_free(&frame);\n            }else{\n                is->audio_buf_ = NULL;\n                is->audio_buf_size = 512;\n            }\n        }\n        len1 = is->audio_buf_size - is->audio_buf_index;\n        if(len1 > len){\n            len1 = len;\n        }\n        if(!is->audio_buf_){\n            memset(stream, 0, len1);\n        }else {\n            memcpy(stream, is->audio_buf_ + is->audio_buf_index, len1);\n        }\n        len -= len1;\n        stream += len1;\n        is->audio_buf_index += len1;\n    }\n    //设置时钟\n    if(is->pts_ != AV_NOPTS_VALUE){\n        double pts = is->pts_ * av_q2d(is->time_base_);\n        av_log(NULL, AV_LOG_INFO, \"audio pts:%0.3lf\\n\", pts);\n        is->avsync_->SetClock(pts);\n    }\n}\n```\n\n- 视频输出\n`videooutput` 在主线成里调用videooutput的一个循环输出的方法，实现视频的渲染，但这样的方式肯定是不好的，总之就先这样吧(ㄟ( ▔, ▔ )ㄏ)\n\n```cpp\nint VideoOutput::MainLoop()\n{\n    SDL_Event event;\n    while(true){\n        RefreshLoopWaitEvent(&event);\n        switch(event.type){\n        case SDL_KEYDOWN:\n            if(event.key.keysym.sym == SDLK_ESCAPE){\n                av_log(NULL, AV_LOG_INFO, \"esc key down\");\n                return 0;\n            }\n            break;\n        case SDL_QUIT:\n            av_log(NULL, AV_LOG_INFO, \"SDL_QUIT\");\n            return 0;\n        default:\n            break;\n        }\n    }\n    return 0;\n}\n\n```\n\nRefreshLoopWaitEvent()这个函数的设计主要是在等待事件的同时完成音视频的对齐，由于是视频对齐音频，在视频质量很高的情况下，会出现视频跟不上音频的情况，所以我在主线程里设置了一个等待时间，目的是让解码线程先运行一段时间，让视频播放时提前有一些数据，避免视频解码过慢造成的问题。但这样肯定是不行的，我想到用开多个视频解码线程的方式解决，但这样又会引出许多问题(＞﹏＜)但我相信会在以后的学习中，这将不会是问题(o(*￣︶￣*)o)\n\n#### 音视频同步\n\n`avsync` 音视频的同步有三种方式，这里采用的是视频基于音频的播放，对时间的获取和设置都是通过该类实现的，内容很少我就直接贴出来了\n```cpp\nAVSync(){}\n\nvoid InitClock(){\n    SetClock(NAN); //数学对比是一个无效值\n}\nvoid SetClockAt(double pts, double time){\n    pts_ = pts;\n    pts_drift_ = pts_ - time;\n}\n\ndouble GetClock(){\n    double time = GetMicroseconds() / 1000000.0;\n    return pts_drift_ + time;\n}\n\nvoid SetClock(double pts){\n    double time = GetMicroseconds() / 1000000.0; //ui->s\n    SetClockAt(pts, time);\n}\n\ntime_t GetMicroseconds(){\n    system_clock::time_point time_point_new = system_clock::now();\n    system_clock::duration duration = time_point_new.time_since_epoch();\n\n    time_t us = duration_cast<microseconds>(duration).count();\n    return us;\n}\n\ndouble pts_ = 0;\ndouble pts_drift_ = 0;\n```\n\n#### 主函数\n```cpp\n#define SDL_MAIN_HANDLED\n#include \"demuxthread.h\"\n#include \"decodethread.h\"\n#include \"audiooutput.h\"\n#include \"videooutput.h\"\n\nusing namespace std;\n\nchar err2str[256] = {0};\n\nint main(int argc, char *argv[])\n{\n    int ret = 0;\n    av_log_set_level(AV_LOG_INFO);\n    if(argc < 2){\n        av_log(NULL, AV_LOG_ERROR, \"argc < 2!\\n\");\n        return -1;\n    }\n    //创建音视频队列\n    AVPacketQueue audio_packet_queue;\n    AVPacketQueue video_packet_queue;\n\n    AVFrameQueue audio_frame_queue;\n    AVFrameQueue video_frame_queue;\n\n    AVSync avsync;\n    avsync.InitClock();\n\n    //1. 解复用线程\n    DemuxThread *demux_thread = new DemuxThread(&audio_packet_queue, &video_packet_queue);\n    ret = demux_thread->Init(argv[1]);\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"demux_thread.Init(argv[1]) failed!\\n\");\n        return -1;\n    }\n\n    ret = demux_thread->Start();\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"demux_thread.Start() failed!\\n\");\n        return -1;\n    }\n\n    //2. 解码线程\n    DecodeThread *video_decode_thread = new DecodeThread(&video_packet_queue, &video_frame_queue);\n    ret = video_decode_thread->Init(demux_thread->VideoCodecParameters());\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"video_decode_thread->Init() failed!\\n\");\n        return -1;\n    }\n    ret = video_decode_thread->Start();\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"video_decode_thread->Start() failed!\\n\");\n        return -1;\n    }\n\n    DecodeThread *audio_decode_thread = new DecodeThread(&audio_packet_queue, &audio_frame_queue);\n    ret = audio_decode_thread->Init(demux_thread->AudioCodecParameters());\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"audio_decode_thread->Init() failed!\\n\");\n        return -1;\n    }\n    ret = audio_decode_thread->Start();\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"audio_decode_thread->Start() failed!\\n\");\n        return -1;\n    }\n\n    //休眠 这里预先解码一些音视频的数据，尽量防止音频的播放速度大于视频的播放速度导致视频播放跟不上音频播放的问题 但这样的解决方式肯定是不够的，可能需要多线程解码的方式\n    std::this_thread::sleep_for(std::chrono::milliseconds(3 * 1000));\n\n    // 初始化audio输出\n    AudioParams audio_params = {0};\n    memset(&audio_params, 0, sizeof(AudioParams));\n    audio_params.channles = demux_thread->AudioCodecParameters()->ch_layout.nb_channels;\n    audio_params.channle_layout = demux_thread->AudioCodecParameters()->ch_layout;\n    audio_params.fmt = (enum AVSampleFormat)demux_thread->AudioCodecParameters()->format;\n    audio_params.freq = demux_thread->AudioCodecParameters()->sample_rate;\n    audio_params.frame_size = demux_thread->AudioCodecParameters()->frame_size;\n\n    AudioOutput *audio_output = new AudioOutput(&avsync, demux_thread->AudioStreamTime(), audio_params, &audio_frame_queue);\n    ret = audio_output->Init();\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"audio_output->Init() failed!\\n\");\n        return -1;\n    }\n\n    // 初始化视频输出\n    VideoOutput *video_output = new VideoOutput(&avsync, demux_thread->VideoStreamTime(),\n                                                &video_frame_queue,\n                                                demux_thread->VideoCodecParameters()->width,\n                                                demux_thread->VideoCodecParameters()->height,\n                                                640, 480);\n    ret = video_output->Init();\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"video_output->Init() failed!\\n\");\n        return -1;\n    }\n    video_output->MainLoop(); //程序会停在这里直到退出\n\n    //休眠\n    //std::this_thread::sleep_for(std::chrono::milliseconds(120 * 1000));\n\n__END:\n    demux_thread->Stop();\n    delete demux_thread;\n\n    audio_decode_thread->Stop();\n    delete audio_decode_thread;\n    video_decode_thread->Stop();\n    delete video_decode_thread;\n\n    audio_output->DeInit();\n    delete audio_output;\n\n    av_log(NULL, AV_LOG_INFO, \"main finished\\n\");\n\n    return 0;\n}\n\n```","tags":["FFmpeg","SDL","多线程","音视频同步","队列"],"categories":["音视频"]},{"title":"SDL与FFmpeg结合实现简单的视频播放器","url":"/2024/12/17/SDL与FFmpeg结合实现简单的视频播放器/","content":"\n用FFmpeg与SDL写了一个简单的音视频播放器，但还有很大的问题。首先并没有使用多线程，音视频的解码渲染操作都在主线程中，导致在同时播放视频的时候，音频的播放会出现断断续续的情况，把视频的播放关闭就不会出现问题。还有就是队列的实现，目前只有音频使用了队列，而视频则是直接播放渲染, 要使用队列的话就不得不实现多线程了。之后还有音视频的同步等等，这些都是需要改进的点。不过我相信在后面的学习中都会解决的<(￣︶￣)↗[GO!]\n\n这里就直接放源码了(○｀ 3′○)\n\n```c\n#include <stdio.h>\n#include <windows.h>\n#include <SDL.h>\n\n#include <libavutil/log.h>\n#include <libavutil/avutil.h>\n#include <libavutil/fifo.h>\n#include <libavformat/avformat.h>\n#include <libavcodec/avcodec.h>\n#include <libswresample/swresample.h>\n\n#define AUDIO_BUFFER_SIZE 1024\n\ntypedef struct _PacketQueue{\n    AVFifo *pkts;\n    int nb_packets;\n    int size;\n    int64_t duration;\n\n    SDL_mutex *mutex;\n    SDL_cond *cond;\n}PacketQueue;\n\ntypedef struct _VideoState{\n    AVCodecContext *avctx;\n    AVPacket *pkt;\n    AVFrame *frame;\n    SDL_Texture *texture;\n\n    struct SwrContext *swr_ctx;\n\n    uint8_t *audio_buf;\n    int audio_buf_size;\n    int audio_buf_index;\n\n    PacketQueue audioq;\n    PacketQueue videoq;\n}VideoState;\n\n\ntypedef struct MyPacketEle{\n    AVPacket *pkt;\n}MyPacketEle;\n\n\nstatic int w_width = 640;\nstatic int w_height = 480;\n\nstatic SDL_Window *win = NULL;\nstatic SDL_Renderer *renderer = NULL;\n\nstatic int packet_queue_init(PacketQueue *q){\n    memset(q, 0, sizeof(PacketQueue));\n    q->pkts = av_fifo_alloc2(1, sizeof(MyPacketEle), AV_FIFO_FLAG_AUTO_GROW);\n    if(!q->pkts){\n        return AVERROR(ENOMEM);\n    }\n\n    q->mutex = SDL_CreateMutex();\n    if(!q->mutex){\n        return AVERROR(ENOMEM);\n    }\n\n    q->cond = SDL_CreateCond();\n    if(!q->cond){\n        return AVERROR(ENOMEM);\n    }\n}\n\nstatic int packet_queue_put_priv(PacketQueue *q, AVPacket *pkt){\n    MyPacketEle mypkt;\n    int ret;\n    mypkt.pkt = pkt;\n    ret = av_fifo_write(q->pkts, &mypkt, 1);\n    if(ret < 0){\n        return ret;\n    }\n    q->nb_packets++;\n    q->size += mypkt.pkt->size + sizeof(mypkt);\n    q->duration += mypkt.pkt->duration;\n    SDL_CondSignal(q->cond);\n    return 0;\n}\n\nstatic int packet_queue_put(PacketQueue *q, AVPacket *pkt){\n    AVPacket *pkt1;\n    int ret = -1;\n    pkt1 = av_packet_alloc();\n    if(!pkt1){\n        av_packet_unref(pkt1);\n        return -1;\n    }\n    av_packet_move_ref(pkt1, pkt);\n    SDL_LockMutex(q->mutex);\n\n    ret = packet_queue_put_priv(q, pkt1);\n    SDL_UnlockMutex(q->mutex);\n\n    if(ret < 0){\n        av_packet_free(&pkt1);\n    }\n\n    return ret;\n}\n\nstatic int packet_queue_get(PacketQueue *q, AVPacket *pkt, int block){\n    MyPacketEle mypkt;\n    int ret = 1;\n\n    SDL_LockMutex(q->mutex);\n    while(1){\n        if(av_fifo_read(q->pkts, &mypkt, 1) >= 0){\n            q->nb_packets--;\n            q->size -= mypkt.pkt->size + sizeof(mypkt);\n            q->duration -=mypkt.pkt->duration;\n            av_packet_move_ref(pkt, mypkt.pkt);\n            av_packet_free(&mypkt.pkt);\n            ret = 1;\n            break;\n        }else if(!block){\n            ret = 0;\n            break;\n        }else{\n            SDL_CondWait(q->cond, q->mutex);\n        }\n    }\n    SDL_UnlockMutex(q->mutex);\n    return ret;\n}\n\nstatic void packet_queue_flush(PacketQueue *q){\n    MyPacketEle mypkt;\n    SDL_LockMutex(q->mutex);\n    while(av_fifo_read(q->pkts, &mypkt, 1) > 0){\n        av_packet_free(&mypkt.pkt);\n    }\n    q->nb_packets = 0;\n    q->size = 0;\n    q->duration = 0;\n    SDL_UnlockMutex(q->mutex);\n}\n\nstatic void packet_queue_destroy(PacketQueue *q){\n    packet_queue_flush(q);\n    av_fifo_freep2(&q->pkts);\n    SDL_DestroyMutex(q->mutex);\n    SDL_DestroyCond(q->cond);\n}\n\nstatic void render(VideoState *is){\n    SDL_UpdateYUVTexture(is->texture, NULL, \n    is->frame->data[0], is->frame->linesize[0], \n    is->frame->data[1], is->frame->linesize[1], \n    is->frame->data[2], is->frame->linesize[2]);\n\n    SDL_RenderClear(renderer);\n    SDL_RenderCopy(renderer, is->texture, NULL, NULL);\n    SDL_RenderPresent(renderer);\n}\n\nstatic int decode(VideoState *is){\n    int ret = -1;\n    ret = avcodec_send_packet(is->avctx, is->pkt);\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"Failed to send pkt to video decoder!\\n\");\n        goto __OUT;\n    }\n    while(ret >= 0){\n        ret = avcodec_receive_frame(is->avctx, is->frame);\n        if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n            ret = 0;\n            goto __OUT;\n        }else if(ret < 0){\n            ret = -1;\n            goto __OUT;\n        }\n        render(is);\n    }\n__OUT:\n    return ret;\n}\n\nstatic int audio_decode_frame(VideoState *ais){\n    int ret = -1;\n    int len2;\n\n    int data_size = 0;\n\n    AVPacket pkt;\n    while(1){\n        if(packet_queue_get(&ais->audioq, &pkt, 1) < 0){\n            return -1;\n        }\n        ret = avcodec_send_packet(ais->avctx, &pkt);\n        if(ret < 0){\n            av_log(ais->avctx, AV_LOG_ERROR, \"Failed to send pkt to audio decoder!\\n\");\n            goto __OUT;\n        }\n        while(ret >= 0){\n            ret = avcodec_receive_frame(ais->avctx, ais->frame);\n            if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                break;\n            }else if(ret < 0){\n                av_log(ais->avctx, AV_LOG_ERROR, \"Failed to receive frame from audio decoder!\\n\");\n                goto __OUT;\n            }\n\n            if(!ais->swr_ctx){\n                AVChannelLayout in_ch_layout, out_ch_layout;\n                av_channel_layout_copy(&in_ch_layout, &ais->avctx->ch_layout);\n                av_channel_layout_copy(&out_ch_layout, &ais->avctx->ch_layout);\n                if(ais->avctx->sample_fmt != AV_SAMPLE_FMT_S16){\n                    swr_alloc_set_opts2(&ais->swr_ctx, &out_ch_layout, AV_SAMPLE_FMT_S16, ais->avctx->sample_rate, &in_ch_layout, ais->avctx->sample_fmt, ais->avctx->sample_rate, 0, NULL);\n                    swr_init(ais->swr_ctx);\n                }\n            }\n\n            if(ais->swr_ctx){\n                const uint8_t **in = (const uint8_t **)ais->frame->extended_data;\n                uint8_t **out = &ais->audio_buf;\n                int out_cont = ais->frame->nb_samples + 512;\n                int in_cont = ais->frame->nb_samples;\n\n                int out_size = av_samples_get_buffer_size(NULL, ais->frame->ch_layout.nb_channels, out_cont, AV_SAMPLE_FMT_S16, 0);\n                av_fast_malloc(&ais->audio_buf, &ais->audio_buf_size, out_size);\n\n                len2 = swr_convert(ais->swr_ctx, out, out_cont, in, in_cont);\n                data_size = len2 * ais->frame->ch_layout.nb_channels * av_get_bytes_per_sample(AV_SAMPLE_FMT_S16);\n            }else {\n                ais->audio_buf = *(ais->frame->extended_data);\n                data_size = av_samples_get_buffer_size(NULL, ais->frame->ch_layout.nb_channels, ais->frame->nb_samples, ais->frame->format, 1);\n            }\n\n            av_packet_unref(&pkt);\n            av_frame_unref(ais->frame);\n\n            return data_size;\n        }\n    }\n__OUT:\n    return ret;\n}\n\nstatic void sdl_audio_callback(void *userdata, Uint8 *stream, int len){\n    int audio_size = 0, len1 = 0;\n    VideoState *ais = (VideoState*)userdata;\n\n    SDL_memset(stream, 0, len);\n    while(len > 0){\n        if(ais->audio_buf_index >= ais->audio_buf_size){\n            audio_size = audio_decode_frame(ais);\n            if(audio_size < 0){\n                ais->audio_buf_size = AUDIO_BUFFER_SIZE;\n                ais->audio_buf = NULL;\n            }else {\n                ais->audio_buf_size = audio_size;\n            }\n            ais->audio_buf_index = 0;\n        }\n        len1 = ais->audio_buf_size - ais->audio_buf_index;\n        if(len1 > len){\n            len1 = len;\n        }\n        if(ais->audio_buf){\n            memcpy(stream, (uint8_t *)ais->audio_buf + ais->audio_buf_index, len1); \n            //SDL_MixAudio(stream, ais->audio_buf, len1, SDL_MIX_MAXVOLUME);\n        }else {\n            memset(stream, 0, len1);\n        }\n        len -= len1;\n        stream += len1;\n        ais->audio_buf_index += len1;\n    }\n}\n\nint APIENTRY WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow) {\n    //1. 设置参数\n    int ret = -1;\n    int vIdx = -1, aIdx = -1;\n\n    SDL_Texture *texture = NULL;\n    SDL_Event event;\n\n    Uint32 pixformat = 0;\n    int video_width = 0;\n    int video_height = 0;\n\n    VideoState *vis = NULL;\n    VideoState *ais = NULL;\n\n    SDL_AudioSpec wanted_spec, spec;\n\n    AVFormatContext *FmtCtx = NULL;\n    AVStream *vinstream = NULL;\n    AVStream *ainstream = NULL;\n\n    const AVCodec *vdec = NULL;\n    AVCodecContext *vctx = NULL;\n    const AVCodec *adec = NULL;\n    AVCodecContext *actx = NULL;\n\n    AVPacket *vpkt = NULL;\n    AVFrame *vframe = NULL;\n    AVPacket *apkt = NULL;\n    AVFrame *aframe = NULL;\n    AVPacket *pkt = NULL;\n\n    char *src = NULL;\n    src = strtok(lpCmdLine, \" \");\n    if(!src){\n        printf(\"Error: Expected one parameters.\\n\");\n        goto __END;\n    }\n    \n    vis = av_mallocz(sizeof(VideoState));\n    if(!vis){\n        av_log(NULL, AV_LOG_ERROR, \"NO MEMORY!\\n\");\n        goto __END;\n    }\n    ais = av_mallocz(sizeof(VideoState));\n    if(!ais){\n        av_log(NULL, AV_LOG_ERROR, \"NO MEMORY!\\n\");\n        goto __END;\n    }\n\n    // // strtok 用于分割命令行字符串，默认分隔符为空格\n    // src = strtok(lpCmdLine, \" \");\n    // if (src != NULL) {\n    //     dst = strtok(NULL, \" \");\n    // }\n\n    // // 检查是否成功解析了两个参数\n    // if (src != NULL && dst != NULL) {\n    //     printf(\"Parameter 1: %s\\n\", src);\n    //     printf(\"Parameter 2: %s\\n\", dst);\n    // } else {\n    //     printf(\"Error: Expected two parameters.\\n\");\n    // }\n\n    //2. 初始化SDL，并创建窗口和Render\n    if (SDL_Init(SDL_INIT_AUDIO | SDL_INIT_VIDEO)) {\n        printf(\"初始化 SDL 失败！\\n\");\n        return ret;\n    }\n\n    win = SDL_CreateWindow(\"YUV Player\", \n                            SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, \n                            w_width, w_height, SDL_WINDOW_OPENGL|SDL_WINDOW_RESIZABLE);\n    if(!win){\n        printf(\"Failed to create window, %s\\n\", SDL_GetError());\n        goto __END;\n    }\n    renderer = SDL_CreateRenderer(win, -1, 0);\n\n    //3. 打开多媒体文件，并获取流信息\n    if((ret = avformat_open_input(&FmtCtx, src, NULL, NULL)) < 0){\n        av_log(NULL, AV_LOG_ERROR, \"%s\\n\", av_err2str(ret));\n        goto __END;\n    }\n    ret = avformat_find_stream_info(FmtCtx, NULL);\n    if(ret < 0){\n        av_log(NULL, AV_LOG_ERROR, \"%s\\n\", av_err2str(ret));\n        goto __END;\n    }\n\n    //4. 查找流\n    for(int i = 0; i < FmtCtx->nb_streams; i++){\n        if(FmtCtx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO && vIdx < 0){\n            vIdx = i;\n        }\n        if(FmtCtx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_AUDIO && aIdx < 0){\n            aIdx = i;\n        }\n        if(vIdx > -1 && aIdx > -1){\n            break;\n        }\n    }\n    if(vIdx < 0 || aIdx < 0){\n        av_log(NULL, AV_LOG_ERROR, \"视频有问题!\");\n        goto __END;\n    }\n\n    ainstream = FmtCtx->streams[aIdx];\n    vinstream = FmtCtx->streams[vIdx];\n\n    //5. 根据流中的codec_id，获得视频解码器\n    vdec = avcodec_find_decoder(vinstream->codecpar->codec_id);\n    if(!vdec){\n        av_log(NULL, AV_LOG_ERROR, \"Could not find codec\");\n        goto __END;\n    }\n    //6. 创建解码器上下文\n    vctx = avcodec_alloc_context3(vdec);\n    if(!vctx){\n        av_log(NULL, AV_LOG_ERROR, \"NO MEMRORY\\n\");\n        goto __END;\n    }\n    //7. 从多媒体文件中拷贝解码器参数到解码器上下文中\n    ret = avcodec_parameters_to_context(vctx, vinstream->codecpar);\n    if(ret < 0){\n        av_log(vctx, AV_LOG_ERROR, \"Could not copy codecpar to codec ctx!\\n\");\n        goto __END;\n    }\n\n    //8. 绑定解码器上下文\n    ret = avcodec_open2(vctx, vdec, NULL);\n    if(ret < 0){\n        av_log(vctx, AV_LOG_ERROR, \"Don't open codec:%s\\n\", av_err2str(ret));\n        goto __END;\n    }\n\n    //9. 根据流中的codec_id，获得音频解码器\n    adec = avcodec_find_decoder(ainstream->codecpar->codec_id);\n    if(!adec){\n        av_log(NULL, AV_LOG_ERROR, \"Could not find codec\");\n        goto __END;\n    }\n    //10. 创建解码器上下文\n    actx = avcodec_alloc_context3(adec);\n    if(!actx){\n        av_log(NULL, AV_LOG_ERROR, \"NO MEMRORY\\n\");\n        goto __END;\n    }\n    //11. 从多媒体文件中拷贝解码器参数到解码器上下文中\n    ret = avcodec_parameters_to_context(actx, ainstream->codecpar);\n    if(ret < 0){\n        av_log(actx, AV_LOG_ERROR, \"Could not copy codecpar to codec ctx!\\n\");\n        goto __END;\n    }\n    //12. 绑定解码器上下文\n    ret = avcodec_open2(actx, adec, NULL);\n    if(ret < 0){\n        av_log(actx, AV_LOG_ERROR, \"Don't open codec:%s\\n\", av_err2str(ret));\n        goto __END;\n    }\n\n    //13. 设置音视频纹理参数\n    video_width = vctx->width;\n    video_height = vctx->height;\n    pixformat = SDL_PIXELFORMAT_IYUV;\n    texture = SDL_CreateTexture(renderer, pixformat, SDL_TEXTUREACCESS_STREAMING, video_width, video_height);\n\n    \n    vpkt = av_packet_alloc();\n    vframe = av_frame_alloc();\n\n    apkt = av_packet_alloc();\n    aframe = av_frame_alloc();\n\n    pkt = av_packet_alloc();\n\n    vis->texture = texture;\n    vis->avctx = vctx;\n    vis->pkt = vpkt;\n    vis->frame = vframe;\n    packet_queue_init(&vis->videoq);\n\n    ais->texture = texture;\n    ais->avctx = actx;\n    ais->pkt = apkt;\n    ais->frame = aframe;\n    packet_queue_init(&ais->audioq);\n\n    wanted_spec.freq = actx->sample_rate;\n    wanted_spec.format = AUDIO_S16SYS;\n    wanted_spec.channels = actx->ch_layout.nb_channels;\n    wanted_spec.silence = 0;\n    wanted_spec.samples = AUDIO_BUFFER_SIZE;\n    wanted_spec.callback = sdl_audio_callback;\n    wanted_spec.userdata = (void *)ais;\n\n    if(SDL_OpenAudio(&wanted_spec, NULL) < 0){\n        av_log(NULL, AV_LOG_ERROR, \"Failed to open audio device!\\n\");\n        goto __END;\n    }\n\n    SDL_PauseAudio(0);\n\n    //14. 从多媒体文件中读取数据，进行解码\n    while(1){\n        av_read_frame(FmtCtx, pkt);\n        //15. 对解码后的视频数据进行渲染\n        if(pkt->stream_index == vIdx){\n            vis->pkt = pkt;\n            decode(vis);\n            // if(packet_queue_put(&vis->videoq, pkt) < 0){\n            //     printf(\"向队列添加音频数据失败!\");\n            // }\n        }else if(pkt->stream_index == aIdx){\n            if(packet_queue_put(&ais->audioq, pkt) < 0){\n                printf(\"向队列添加音频数据失败!\");\n            }\n        }else {\n            av_packet_unref(pkt);\n        }\n        //延迟 1 毫秒，防止 CPU 占用过高\n        SDL_Delay(1);\n\n        //16. 处理SDL事件\n        SDL_PollEvent(&event);\n        switch(event.type){\n            case SDL_QUIT:\n                exit(-1);\n            default:\n                break;\n        }\n        av_packet_unref(vpkt);\n    }\n\n    vis->pkt = NULL;\n    decode(vis);\n    packet_queue_put(&ais->audioq, NULL);\n\n__END:\n    //17. 收尾，释放资源\n    if(vframe){\n        av_frame_free(&vframe);\n    }\n    if(vpkt){\n        av_packet_free(&vpkt);\n    }\n    if(aframe){\n        av_frame_free(&aframe);\n    }\n    if(apkt){\n        av_packet_free(&apkt);\n    }\n    if(vctx){\n        avcodec_free_context(&vctx);\n    }\n    if(FmtCtx){\n        avformat_close_input(&FmtCtx);\n    }\n    if(win){\n        SDL_DestroyWindow(win);\n    }\n    if(renderer){\n        SDL_DestroyRenderer(renderer);\n    }\n    if(texture){\n        SDL_DestroyTexture(texture);\n    }\n    if(ais){\n        av_free(ais);\n    }\n    if(vis){\n        av_free(vis);\n    }\n    SDL_Quit();\n    return ret;\n}\n```","tags":["FFmpeg","SDL"],"categories":["音视频"]},{"title":"SDL初探","url":"/2024/12/14/SDL初探/","content":"\n学习了一下SDL，跟着教学用SDL实现了以下的简单程序\n这里入口函数被设置为了WinMain，是因为SDL对入口函数的要求，可以通过加入`#define SDL_MAIN_HANDLED`来阻止SDL重写main函数。\n\n### 随机加载红色方块\n\n```c\n#include <stdio.h>\n#include <windows.h>\n#include <SDL.h>\n\nint APIENTRY WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow)\n{\n    int quit = 1;\n    SDL_Event event;\n    SDL_Window *window = NULL;\n    SDL_Renderer *render = NULL;\n    SDL_Texture *texture = NULL;\n\n    SDL_Rect rect;\n    rect.w = 30;\n    rect.h = 30;\n\n    // 初始化 SDL 视频子系统\n    if (SDL_Init(SDL_INIT_VIDEO) != 0) {\n        printf(\"SDL_Init failed: %s\\n\", SDL_GetError());\n        return -1;\n    }\n\n    // 创建窗口\n    window = SDL_CreateWindow(\"Hello World\", SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED, 640, 360, SDL_WINDOW_SHOWN);\n    if (!window) {\n        printf(\"Failed to Create Window: %s\\n\", SDL_GetError());\n        goto __EXIT;\n    }\n\n    // 创建渲染器\n    render = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED);\n    if (!render) {\n        printf(\"Failed to Create Render: %s\\n\", SDL_GetError());\n        goto __EXIT;\n    }\n\n    // // 设置渲染颜色为红色\n    // SDL_SetRenderDrawColor(render, 255, 0, 0, 255);\n\n    // // 用设置好的颜色刷新屏幕\n    // SDL_RenderClear(render);\n\n    // // 让显示器将新屏幕展示出来\n    // SDL_RenderPresent(render);\n\n    texture = SDL_CreateTexture(render, SDL_PIXELFORMAT_RGBA8888, SDL_TEXTUREACCESS_TARGET, 640, 360);\n    if(!texture){\n        printf(\"Failed to Create Textrue!\");\n        goto __EXIT;\n    }\n    do{\n        SDL_PollEvent(&event);\n        switch(event.type){\n        case SDL_QUIT:\n            quit = 0;\n            break;\n        default:\n            printf(\"event type is %d\", event.type);\n        }\n        rect.x = rand() % 640;\n        rect.y = rand() % 360;\n        SDL_SetRenderTarget(render, texture);\n        SDL_SetRenderDrawColor(render, 0, 0, 0, 255);\n        SDL_RenderClear(render);\n\n        SDL_RenderDrawRect(render, &rect);\n        SDL_SetRenderDrawColor(render, 255, 0, 0, 255);\n        SDL_RenderFillRect(render, &rect);\n\n        SDL_SetRenderTarget(render, NULL);\n        SDL_RenderCopy(render, texture, NULL, NULL);\n        SDL_RenderPresent(render);\n\n    }while(quit);\n\n    // // 等待3秒钟\n    // SDL_Delay(3000);\n\n\n__EXIT:\n    if(window){\n        // 销毁窗口\n        SDL_DestroyWindow(window);\n    }\n    if(render) {\n        // 销毁渲染器\n        SDL_DestroyRenderer(render);\n    }\n    if(texture){\n        // 销毁纹理器\n        SDL_DestroyTexture(texture);\n    }\n\n    SDL_Quit();\n\n    return 0;\n}\n```\n\n### 实现PCM的播放\n\n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <windows.h>\n#include <SDL.h>\n\n#define BLOCK_SIZE 4096000  // 定义音频缓冲区大小\n\nstatic size_t buffer_len = 0;  // 当前缓冲区的数据长度\nstatic Uint8 *audio_pos = NULL;  // 当前音频数据的位置\nstatic Uint8 *audio_buf = NULL;  // 存放音频数据的缓冲区\n\n// SDL 音频回调函数，处理音频播放数据\nvoid read_audio_data(void *udata, Uint8 *stream, int len) {\n    if (buffer_len == 0) {\n        return;  // 如果没有数据，直接返回\n    }\n\n    // 清空音频输出流\n    SDL_memset(stream, 0, len);\n\n    // 确保不会读取超过缓冲区的数据\n    len = (len < buffer_len) ? len : buffer_len;\n\n    // 将音频数据混合到输出流中\n    SDL_MixAudio(stream, audio_pos, len, SDL_MIX_MAXVOLUME);\n\n    // 更新音频数据位置和剩余数据长度\n    audio_pos += len;\n    buffer_len -= len;\n}\n\nint APIENTRY WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow) {\n    int ret = -1;\n    char *path = \"./output.pcm\";  // PCM 文件路径\n    FILE *audio_fd = NULL;\n\n    // 初始化 SDL\n    if (SDL_Init(SDL_INIT_AUDIO | SDL_INIT_TIMER)) {\n        printf(\"初始化 SDL 失败！\\n\");\n        return ret;\n    }\n\n    // 打开 PCM 文件 并以二进制的模式读取，避免数据的错误读取\n    audio_fd = fopen(path, \"rb\");\n    if (!audio_fd) {\n        printf(\"打开 PCM 文件失败！\\n\");\n        goto __FAIL;\n    }\n\n    // 为音频数据分配内存缓冲区\n    audio_buf = (Uint8*)malloc(BLOCK_SIZE);\n    if (!audio_buf) {\n        printf(\"分配内存失败！\\n\");\n        goto __FAIL;\n    }\n\n    // 设置 SDL 音频规格\n    SDL_AudioSpec spec;\n    spec.freq = 48000;       // 设置采样率为 48000Hz\n    spec.channels = 2;       // 设置为立体声 (2 通道)\n    spec.format = AUDIO_S16SYS; // 设置音频格式为 16-bit 有符号整数\n    spec.silence = 0;        // 设置静音值为 0\n    spec.callback = read_audio_data;  // 设置回调函数\n    spec.userdata = NULL;    // 未使用的用户数据\n\n    // 打开音频设备\n    if (SDL_OpenAudio(&spec, NULL) < 0) {\n        printf(\"打开音频设备失败！\\n\");\n        goto __FAIL;\n    }\n\n    // 启动音频播放\n    SDL_PauseAudio(0);\n\n    // 音频数据播放循环\n    do {\n        // 从 PCM 文件中读取数据到缓冲区\n        buffer_len = fread(audio_buf, 1, BLOCK_SIZE, audio_fd);\n        audio_pos = audio_buf;\n\n        // 等待直到音频回调函数处理完数据\n        while (buffer_len > 0) {\n            SDL_Delay(10);  // 延迟 10 毫秒，防止 CPU 占用过高\n        }\n\n    } while (buffer_len != 0);  // 直到没有更多数据\n\n    // 关闭音频设备\n    SDL_CloseAudio();\n    ret = 0;\n\n__FAIL:\n    // 释放资源\n    if (audio_fd) {\n        fclose(audio_fd);\n    }\n    if (audio_buf) {\n        free(audio_buf);\n    }\n    SDL_Quit();\n    return ret;\n}\n\n```","tags":["SDL"],"categories":["音视频"]},{"title":"实现FFmpeg的视频转图片(BMP)","url":"/2024/12/13/实现FFmpeg的视频转图片(BMP)/","content":"\n结构体信息可以看这里\nhttps://learn.microsoft.com/en-us/windows/win32/api/wingdi/ns-wingdi-bitmapfileheader\n\n这里最容易出错的地方就是这里的\n#pragma pack(push, 1) // 确保结构体对齐方式与 BMP 格式一致\n#pragma pack(pop)\n\n对这个的讲解我这里直接引用了AI的(讲的是真的好（〃｀ 3′〃）)\n### `#pragma pack(push, 1)` 和 `#pragma pack(pop)` 的作用\n\n这两个编译指令用来控制 **结构体成员的对齐方式**，特别是设置如何在内存中排列结构体的数据。  \n\n#### 默认对齐方式\n默认情况下，编译器会对结构体成员进行对齐，以优化内存访问的性能。  \n例如，如果一个结构体成员是 `uint32_t` 类型（4 字节），它通常会对齐到 **4 字节边界**。这样会导致结构体中可能出现一些额外的 **填充字节**（padding），以保证对齐。\n\n例如：\n```c\ntypedef struct {\n    uint8_t a;  // 1 字节\n    uint32_t b; // 4 字节\n} Example;\n\n默认情况下，这个结构体的内存布局是：\n| a | 填充 (3 字节) | b | => 总大小为 8 字节\n```\n\n#### `#pragma pack(push, 1)`\n当使用 `#pragma pack(push, 1)` 时，结构体的成员按 **1 字节对齐**。也就是说，成员会紧密排列，不会插入填充字节。\n\n改用 `#pragma pack(push, 1)` 的情况：\n```c\n#pragma pack(push, 1)\ntypedef struct {\n    uint8_t a;  // 1 字节\n    uint32_t b; // 4 字节\n} Example;\n#pragma pack(pop)\n\n内存布局：\n| a | b | => 总大小为 5 字节\n```\n\n#### 为什么在处理 BMP 文件时使用？\nBMP 文件头和信息头的格式是严格定义的，它们在文件中的每一部分都需要紧密排列，不能有填充字节。  \n例如：\n- `BMPFileHeader` 的大小必须是 14 字节。\n- `BMPInfoHeader` 的大小必须是 40 字节。\n\n如果使用默认对齐方式，结构体可能会包含填充字节，导致文件格式不符合规范。使用 `#pragma pack(push, 1)` 可以确保数据结构的内存布局与 BMP 文件的定义完全一致。\n\n#### `#pragma pack(pop)`\n这个指令用来恢复对齐方式到之前的状态。  \n`#pragma pack(push, 1)` 改变了对齐方式，因此在定义完需要紧密对齐的结构体后，用 `#pragma pack(pop)` 恢复默认对齐方式，以免影响后续代码的性能。\n\n---\n\n#### 总结\n- `#pragma pack(push, 1)` 确保结构体紧密对齐，无填充字节，适合文件格式的定义。\n- `#pragma pack(pop)` 恢复默认对齐方式，避免全局影响。\n- 在处理像 BMP 这种对数据格式严格要求的场景时，显得尤为重要。\n\n### 将视频转BMP图片的案例\n\n```c\n#pragma pack(push, 1) // 确保结构体对齐方式与 BMP 格式一致\n\n// BMP 文件头\ntypedef struct {\n    uint16_t bfType;      // 文件标识符 ('BM')\n    uint32_t bfSize;      // 文件大小\n    uint16_t bfReserved1; // 保留字段\n    uint16_t bfReserved2; // 保留字段\n    uint32_t bfOffBits;   // 数据偏移量\n} BMPFileHeader;\n\n// BMP 信息头\ntypedef struct {\n    uint32_t biSize;          // 信息头大小\n    int32_t  biWidth;         // 图像宽度\n    int32_t  biHeight;        // 图像高度\n    uint16_t biPlanes;        // 颜色平面数（始终为1）\n    uint16_t biBitCount;      // 每像素位数（24位，BGR）\n    uint32_t biCompression;   // 压缩方式（0表示不压缩）\n    uint32_t biSizeImage;     // 图像数据大小\n    int32_t  biXPelsPerMeter; // 水平分辨率（像素/米）\n    int32_t  biYPelsPerMeter; // 垂直分辨率（像素/米）\n    uint32_t biClrUsed;       // 调色板颜色数（0表示无调色板）\n    uint32_t biClrImportant;  // 重要颜色数（0表示所有颜色重要）\n} BMPInfoHeader;\n\n#pragma pack(pop)\n\nvoid saveBMP(unsigned char* buf, int width, int height, int linesize, const char* name) {\n    FILE *f = fopen(name, \"wb\");\n    if (!f) {\n        fprintf(stderr, \"无法打开文件: %s\\n\", name);\n        return;\n    }\n\n    // 初始化 BMP 文件头\n    BMPFileHeader fileHeader = {\n        .bfType = 0x4D42, // 'BM'\n        .bfSize = 0,      // 稍后填充\n        .bfReserved1 = 0,\n        .bfReserved2 = 0,\n        .bfOffBits = sizeof(BMPFileHeader) + sizeof(BMPInfoHeader) // 数据偏移量\n    };\n\n    // 初始化 BMP 信息头\n    BMPInfoHeader infoHeader = {\n        .biSize = sizeof(BMPInfoHeader),\n        .biWidth = width,\n        .biHeight = height, // 注意：正数表示从下到上存储\n        .biPlanes = 1,\n        .biBitCount = 24,\n        .biCompression = 0,\n        .biSizeImage = 0,   // 稍后填充\n        .biXPelsPerMeter = 0,\n        .biYPelsPerMeter = 0,\n        .biClrUsed = 0,\n        .biClrImportant = 0\n    };\n\n    // 计算每行填充的字节数\n    //int row_padded = (width * 3 + 3) & (~3); // 行填充到4字节对齐\n    int image_size = width * height;    // 图像数据总大小\n    fileHeader.bfSize = sizeof(BMPFileHeader) + sizeof(BMPInfoHeader) + image_size;\n    infoHeader.biSizeImage = image_size;\n\n    // 写入文件头和信息头\n    fwrite(&fileHeader, sizeof(BMPFileHeader), 1, f);\n    fwrite(&infoHeader, sizeof(BMPInfoHeader), 1, f);\n\n    // 写入图像数据（从下到上存储）\n    for (int i = height - 1; i >= 0; i--) {\n        fwrite(buf + i * linesize, 1, width * 3, f); // 每个像素3个字节，RGB\n    }\n    fclose(f);\n}\n\n\n\nstatic int decode(AVCodecContext *ctx, AVFrame *frame, AVPacket *pkt, const char* fileName) {\n    int ret = -1;\n    char buf[1024];\n\n    ret = avcodec_send_packet(ctx, pkt);\n    if (ret < 0) {\n        av_log(NULL, AV_LOG_ERROR, \"发送数据包到解码器失败!\\n\");\n        goto _END;\n    }\n\n    while (ret >= 0) {\n        ret = avcodec_receive_frame(ctx, frame);\n        if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) {\n            return 0;\n        } else if (ret < 0) {\n            return -1;\n        }\n\n        // 创建转换上下文，将YUV转换为RGB\n        struct SwsContext *sws_ctx = sws_getContext(\n            frame->width, frame->height, frame->format,\n            frame->width, frame->height, AV_PIX_FMT_BGR24,\n            SWS_BICUBIC, NULL, NULL, NULL\n        );\n        if (!sws_ctx) {\n            av_log(NULL, AV_LOG_ERROR, \"初始化转换上下文失败\\n\");\n            return -1;\n        }\n\n        // 为RGB数据分配内存\n        uint8_t *rgb_data = (uint8_t *)malloc(frame->width * frame->height * 3); // 每个像素3个字节\n        int rgb_linesize = frame->width * 3;\n\n        // 执行转换\n        sws_scale(sws_ctx, (const uint8_t *const *)frame->data, frame->linesize, 0, frame->height, &rgb_data, &rgb_linesize);\n\n        snprintf(buf, sizeof(buf), \"%s-%d.bmp\", fileName, ctx->frame_num);\n        saveBMP(rgb_data, frame->width, frame->height, rgb_linesize, buf);\n\n        free(rgb_data);\n        sws_freeContext(sws_ctx);\n    }\n\n_END:\n    return 0;\n}\n\n\nint main(int argc, char *argv[])\n{\n    int ret = -1;\n    int idx = -1;\n    // 1. 处理参数\n    char* src;  // 输入数据\n    char* dst;  // 输出数据\n\n    AVFormatContext *pFmtCtx = NULL;\n    AVStream *inStream = NULL;\n\n    const AVCodec *codec = NULL;\n    AVCodecContext *ctx = NULL;\n\n    AVPacket *pkt = NULL;\n    AVFrame *frame = NULL;\n\n    av_log_set_level(AV_LOG_DEBUG);\n    if(argc < 3) {\n        av_log(NULL, AV_LOG_INFO, \"参数数量不足!\\n\");\n        exit(-1);\n    }\n\n    src = argv[1];\n    dst = argv[2];\n\n    // 2. 配置输入上下文，打开多媒体文件\n    if((ret = avformat_open_input(&pFmtCtx, src, NULL, NULL)) < 0){\n        av_log(NULL, AV_LOG_ERROR, \"%s\\n\", av_err2str(ret));\n        exit(-1);\n    }\n\n    // 3. 从多媒体文件中找到视频流\n    idx = av_find_best_stream(pFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0);\n    if(idx < 0){\n        av_log(pFmtCtx, AV_LOG_ERROR, \"未找到视频流!\\n\");\n        goto __ERROR;\n    }\n\n    // 4. 查找编码器\n    inStream = pFmtCtx->streams[idx];\n    codec = avcodec_find_decoder(inStream->codecpar->codec_id);\n    if(!codec){\n        av_log(NULL, AV_LOG_ERROR, \"找不到解码器\\n\");\n        goto __ERROR;\n    }\n\n    // 5. 配置编码器上下文\n    ctx = avcodec_alloc_context3(codec);\n    if(!ctx){\n        av_log(NULL, AV_LOG_ERROR, \"内存分配失败\\n\");\n        goto __ERROR;\n    }\n    avcodec_parameters_to_context(ctx, inStream->codecpar);\n\n    // 6. 打开解码器\n    ret = avcodec_open2(ctx, codec, NULL);\n    if(ret < 0){\n        av_log(ctx, AV_LOG_ERROR, \"解码器打开失败: %s\\n\", av_err2str(ret));\n        goto __ERROR;\n    }\n\n    // 7. 创建AVFrame\n    frame = av_frame_alloc();\n    if(!frame){\n        av_log(NULL, AV_LOG_ERROR, \"内存分配失败!\\n\");\n        goto __ERROR;\n    }\n\n    // 8. 创建AVPacket\n    pkt = av_packet_alloc();\n    if(!pkt){\n        av_log(NULL, AV_LOG_ERROR, \"内存分配失败!\\n\");\n        goto __ERROR;\n    }\n\n    // 9. 读取视频数据并保存为图像\n    while(av_read_frame(pFmtCtx, pkt) >= 0){\n        if(pkt->stream_index == idx){\n            decode(ctx, frame, pkt, dst);\n        }\n        av_packet_unref(pkt);\n    }\n\n    decode(ctx, frame, NULL, dst);\n\n    //10. 释放资源\n__ERROR:\n    if(pFmtCtx){\n        avformat_close_input(&pFmtCtx);\n        pFmtCtx = NULL;\n    }\n    if(ctx){\n        avcodec_free_context(&ctx);\n        ctx = NULL;\n    }\n    if(frame){\n        av_frame_free(&frame);\n        frame = NULL;\n    }\n    if(pkt){\n        av_packet_free(&pkt);\n        pkt = NULL;\n    }\n    return 0;\n}\n```","tags":["FFmpeg","BMP"],"categories":["音视频"]},{"title":"用FFmpeg自带的AAC编码器进行编码","url":"/2024/12/11/用FFmpeg自带的AAC编码器进行编码/","content":"\n大体内容和https://stardm.ddns-ip.net/2024/12/01/%E2%91%A2%E5%AE%9E%E7%8E%B0%E7%94%B5%E8%84%91%E9%9F%B3%E9%A2%91%E5%92%8C%E8%A7%86%E9%A2%91%E7%9A%84%E5%BD%95%E5%88%B6%E5%B9%B6%E5%B0%81%E8%A3%85%E6%88%90mp4/ 这一篇的相同，这里主要展示改变有变动的部分\n\n### **音频采集步骤**\n\n这里的序号与`③实现电脑音频和视频的录制并封装成mp4`这篇里的对应\n\n3. **打开AAC编码器**  \n   ```cpp\n   open_encoder(&codec_ctx);\n   ```\n\n4. **配置重采样上下文（主要就是多了这一步）**  \n   ```cpp\n   init_resampler(codec_ctx, &swr_ctx, &src_data, &dst_data);\n   ```\n\n\n8. **配置解码后数据的frame容器及音频帧**\n\n```cpp\n    //获取frame_size\n    const int frame_size = codec_ctx->frame_size;\n    // 初始化帧编号\n    int64_t pts = 0;\n\n    frame->nb_samples = dec_ctx->frame_size;\n    frame->sample_rate = dec_ctx->sample_rate;\n    frame->ch_layout = dec_ctx->ch_layout;\n    frame->format = dec_ctx->sample_fmt;\n\n    av_frame_get_buffer(frame, 0);\n\n    AVFrame *nframe = av_frame_alloc();\n    nframe->nb_samples = codec_ctx->frame_size;\n    nframe->sample_rate = codec_ctx->sample_rate;\n    nframe->ch_layout = codec_ctx->ch_layout;\n    nframe->format = codec_ctx->sample_fmt;\n\n    av_frame_get_buffer(nframe, 0);\n\n    fifo = init_audio_fifo(codec_ctx->sample_fmt, dec_ctx->ch_layout.nb_channels);\n```\n\n9. **用 `av_read_frame()` 读取音频数据并写入文件**\n\n---\n\n### **open_encoder() 的具体实现**\n\n```cpp\nint AudioRecorder::open_encoder(AVCodecContext **codec_ctx) {\n    AVDictionary *options = NULL;\n    const AVCodec *codec = avcodec_find_encoder_by_name(\"aac\"); //这里改成aac\n    if (!codec) {\n        qDebug() << \"编码器 libfdk_aac 未找到\";\n        return 0;\n    }\n\n    // 分配编码器上下文\n    *codec_ctx = avcodec_alloc_context3(codec);\n    if (!*codec_ctx) {\n        qDebug() << \"编码器上下文分配失败\";\n        return 0;\n    }\n\n    // 设置编码器参数\n    AVChannelLayout ch_layout;\n    av_channel_layout_default(&ch_layout, 2);\n    (*codec_ctx)->sample_rate = 48000;\n    (*codec_ctx)->sample_fmt = AV_SAMPLE_FMT_FLTP;  // 样本格式改成FLTP\n    (*codec_ctx)->bit_rate = 128000;\n    if (av_channel_layout_copy(&(*codec_ctx)->ch_layout, &ch_layout) < 0) {\n        qDebug() << \"设置通道布局失败\";\n        return 0;\n    }\n\n    // 打开编码器\n    if (avcodec_open2(*codec_ctx, codec, &options) < 0) {\n        qDebug() << \"编码器打开失败\";\n        return 0;\n    }\n\n    return 1;\n}\n```\n\n---\n\n### **init_resampler() 的具体实现(这里保持不变)**\n\n```cpp\nint AudioRecorder::init_resampler(AVCodecContext *codec_ctx, SwrContext **swr_ctx, uint8_t ***src_data, uint8_t ***dst_data) {\n    *swr_ctx = swr_alloc();\n    AVChannelLayout in_ch_layout;\n    av_channel_layout_default(&in_ch_layout, 2);  // 立体声布局\n\n    // 配置重采样上下文\n    swr_alloc_set_opts2(swr_ctx,\n                        &codec_ctx->ch_layout,          // 输出通道布局\n                        codec_ctx->sample_fmt,          // 输出采样格式\n                        codec_ctx->sample_rate,         // 输出采样率\n                        &in_ch_layout,                 // 输入通道布局\n                        AV_SAMPLE_FMT_S16,              // 输入采样格式\n                        48000,                         // 输入采样率\n                        0, nullptr);\n\n    if (!*swr_ctx || swr_init(*swr_ctx) < 0) {\n        qDebug() << \"重采样初始化失败\";\n        return 0;\n    }\n\n    // 创建输入输出缓冲区\n    av_samples_alloc_array_and_samples(src_data, nullptr, in_ch_layout.nb_channels, 22050, AV_SAMPLE_FMT_S16, 0);\n    av_samples_alloc_array_and_samples(dst_data, nullptr, 2, 22050, codec_ctx->sample_fmt, 0);\n    \n    return 1;  // 成功\n}\n```\n\n---\n\n### **音频数据读取与写入文件过程**\n\n看官方案例就可以知道我这里写的一塌糊涂(不过我也懒得改了(￣﹃￣))\n\n```cpp\n //开始获取音频帧数据，并进行转换\n    while (flage && av_read_frame(fmt_ctx, &pkt) == 0) {\n        int ret = avcodec_send_packet(dec_ctx, &pkt);\n        if(ret < 0){\n            qDebug() << \"向解码器发送数据包失败!\";\n            return;\n        }\n        while(ret >= 0){\n            ret = avcodec_receive_frame(dec_ctx, frame);\n            if(ret < 0){\n                if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                    break;\n                }\n                qDebug() << \"Error, decoding video frame\";\n                return;\n            }\n\n            swr_convert(swr_ctx, dst_data, nb_samples, frame->data, nb_samples);//添加这一步\n\n            //将样本数据写入 FIFO\n            write_to_fifo(fifo, (const uint8_t**)dst_data, nb_samples); //这里的frame改成dst_data\n            while (av_audio_fifo_size(fifo) >= frame_size) {\n                read_from_fifo(fifo, nframe->data, frame_size);\n                nframe->nb_samples = codec_ctx->frame_size;\n                nframe->pts = pts;\n                ret = avcodec_send_frame(codec_ctx, nframe); //还有这个发送的数据包改成nframe\n                if(ret < 0){\n                    qDebug() << \"向编码器发送数据包失败!\";\n                    return;\n                }\n                while(ret >= 0){\n                    ret = avcodec_receive_packet(codec_ctx, newpkt);\n                    if(ret < 0){\n                        if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                            break;\n                        }\n                        qDebug() << \"Error, encoding video frame\";\n                        return;\n                    }\n                    newpkt->pts = newpkt->dts = pts;\n                    newpkt->stream_index = audio_stream_index;\n                    CallbackFun write = std::bind(&MainWindow::Main_av_interleaved_write_frame, static_cast<MainWindow *>(parent), *ofmt_ctx, newpkt);\n                    write();\n                    av_packet_unref(newpkt);\n                }\n                pts += frame_size;\n            }\n            av_frame_unref(frame);\n        }\n        av_packet_unref(&pkt);\n    }\n```\n之前我这里read_from_fifo(fifo, frame->data, frame_size); 之前偷懒，直接用frame。因为没有重采样，所以frame的data数据又是一致的，也就是没有发生改写操作，这样就不会受引用计数的影响，从而不会报错。但就是这么点细节结果让我在用官方AAC编码器上一直写不对(然后就用libfdk_aac了^(*￣(oo)￣)^)","tags":["FFmpeg","AAC"],"categories":["音视频"]},{"title":"Cloudflare证书的颁发切换为 Google Trust(GTS)","url":"/2024/12/10/Cloudflare证书的颁发切换为 Google Trust(GTS)/","content":"\n在使用cloudflare时发现了SSL的证书安全问题，经过一段时间的折腾，证书安全问题倒是解决了。但是我发现cloudflare里SSL显示的有效证书与我网站的真实证书不一致，所以就找到了这个调用官方API的方法。但调用成功后，我发现网站的实际证书还是不一样(很可能是我一开始瞎搞弄成这样的/(ㄒoㄒ)/~~)。总之静观其变吧，看看它续签的时候会不会换回来...(ノへ￣、)\n\n### 通过 API 改变证书(可能需要代理)\n\n#### 输入（适用于 Mac/Linux）：\n\n```bash\ncurl -sX PATCH \"https://api.cloudflare.com/client/v4/zones/[DOMAIN_ZONE_ID_HERE]/ssl/universal/settings\" -H \"X-Auth-Email: [CLOUDFLARE_EMAIL_HERE]\" -H \"X-Auth-Key: [GLOBAL_API_KEY_HERE]\" -H \"Content-Type: application/json\" --data '{\"certificate_authority\":\"google\"}'\n```\n\n#### 或者（对于 Windows）：\n\n```bash\ncurl -sX PATCH \"https://api.cloudflare.com/client/v4/zones/[DOMAIN_ZONE_ID_HERE]/ssl/universal/settings\" -H \"X-Auth-Email: [CLOUDFLARE_EMAIL_HERE]\" -H \"X-Auth-Key: [GLOBAL_API_KEY_HERE]\" -H \"Content-Type: application/json\" --data \"{\\\"certificate_authority\\\":\\\"google\\\"}\"\n```\n\n`[DOMAIN_ZONE_ID_HERE]` 更改为 Zone ID，也就是控制面板显示的那个\n`[CLOUDFLARE_EMAIL_HERE]` 为 Cloudflare 电子邮件地址\n`[GLOBAL_API_KEY_HERE]` 为全局 API 密钥\n`注意不要包含 \"[\" 和 \"]\"`\n\n返回以下内容代表成功\n\n`{\"result\":{\"enabled\":true,\"certificate_authority\":\"google\"},\"success\":true,\"errors\":[],\"messages\":[]}`\n\n#### 查看当前证书(Linux)\n```bash\nopenssl s_client -connect yourdomain.com:443\n```","tags":["解决"],"categories":["杂项"]},{"title":"用FFmpeg实现一个简单小咖秀","url":"/2024/12/09/用FFmpeg实现一个简单小咖秀/","content":"\n### 核心代码\n\n#### 找其中一个源文件的音频流\n```c\nfor(int i = 0; i < ifmt1->nb_streams; i++){\n    AVStream *instream1 = ifmt1->streams[i];\n    AVStream *outstream = NULL;\n    AVCodecParameters *inCodecPar1 = instream1->codecpar;\n\n    if(inCodecPar1->codec_type == AVMEDIA_TYPE_AUDIO){\n        outstream = avformat_new_stream(ofmt, NULL);\n        if(!outstream){\n            av_log(NULL, AV_LOG_ERROR, \"创建流失败!\");\n            goto __ERROR;\n        }\n        avcodec_parameters_copy(outstream->codecpar, instream1->codecpar);\n        outstream->codecpar->codec_tag = 0;\n        aidx = i;\n        aidx1 = cnt++;\n    }\n}\n```\n\n\n#### 找另一个源文件的视频流\n\n```c\nfor(int i = 0; i < ifmt2->nb_streams; i++){\n    AVStream *instream2 = ifmt2->streams[i];\n    AVStream *outstream = NULL;\n    AVCodecParameters *inCodecPar2 = instream2->codecpar;\n\n    if(inCodecPar2->codec_type == AVMEDIA_TYPE_VIDEO){\n        outstream = avformat_new_stream(ofmt, NULL);\n        if(!outstream){\n            av_log(NULL, AV_LOG_ERROR, \"创建流失败!\");\n            goto __ERROR;\n        }\n        avcodec_parameters_copy(outstream->codecpar, instream2->codecpar);\n        outstream->codecpar->codec_tag = 0;\n        vidx = i;\n        vidx1 = cnt++;\n    }\n}\n```\n\n#### 读取源文件并写入到目标文件\n\n```c\nwhile(av_read_frame(ifmt1, &pkt1) >= 0 || av_read_frame(ifmt2, &pkt2) >= 0){\n    if(pkt1.stream_index == aidx){\n        pkt1.pts = av_rescale_q_rnd(pkt1.pts, ifmt1->streams[aidx]->time_base, ofmt->streams[aidx1]->time_base, (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));\n        pkt1.dts = pkt1.pts;\n        pkt1.stream_index = aidx1;\n        av_interleaved_write_frame(ofmt, &pkt1);\n        av_packet_unref(&pkt1);\n    }\n    if(pkt2.stream_index == vidx){\n        pkt2.pts = av_rescale_q_rnd(pkt2.pts, ifmt2->streams[vidx]->time_base, ofmt->streams[vidx1]->time_base, (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));\n        pkt2.dts = av_rescale_q_rnd(pkt2.dts, ifmt2->streams[vidx]->time_base, ofmt->streams[vidx1]->time_base, (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));\n        pkt2.stream_index = vidx1;\n        av_interleaved_write_frame(ofmt, &pkt2);\n        av_packet_unref(&pkt2);        \n    }\n}\n```","tags":["FFmpeg"],"categories":["音视频"]},{"title":"实现FFmpeg的裁剪","url":"/2024/12/08/实现FFmpeg的裁剪/","content":"\n### 核心代码\n\n#### 起始读入位置偏移到与starttime位置最近的后一个帧\n```cpp\nret = av_seek_frame(pFmtCtx, -1, starttime * AV_TIME_BASE, AVSEEK_FLAG_BACKWARD);\nif(ret < 0){\n    av_log(oFmtCtx, AV_LOG_ERROR, \"%s\", av_err2str(ret));\n    goto __ERROR;\n}\n```\n\n#### 申请存储起始读入帧的内存\n```cpp\n    dts_start_time = av_calloc(pFmtCtx->nb_streams, sizeof(int64_t));\n    pts_start_time = av_calloc(pFmtCtx->nb_streams, sizeof(int64_t));\n```\n\n#### 从源多媒体文件中读取数据到目的文件中\n```cpp\nwhile(av_read_frame(pFmtCtx, &pkt) >= 0){\n\n    .....\n\n    //拿到起始读入帧的pts和dts\n    if(pts_start_time[pkt.stream_index] == -1 && pkt.pts > 0 && pkt.dts > 0){\n        pts_start_time[pkt.stream_index] = pkt.pts;\n        dts_start_time[pkt.stream_index] = pkt.dts;\n    }\n\n    //判断是否到达结束时间\n    if((pkt.pts * av_q2d(pFmtCtx->streams[pkt.stream_index]->time_base)) > endtime){\n        av_log(oFmtCtx, AV_LOG_INFO, \"success!\\n\");\n        break;\n    }\n\n    //与开始的帧相减得到相对的pts和dts\n    pkt.pts -= pts_start_time[pkt.stream_index];\n    pkt.dts -= dts_start_time[pkt.stream_index];\n\n    //在含有B帧的情况，编码器会对视频帧进行重排，会导致选择的第一个视频帧的pts大于dts。\n    //这就导致了pts和dts相同的帧，在处理后的pts会小于dts而发生错误, 但对dts的处理是没有问题的\n    //所以在这里将它重新与dts对齐\n    if(pkt.dts > pkt.pts){\n        pkt.pts = pkt.dts;\n    }\n\n    .....\n\n}\n```","tags":["FFmpeg"],"categories":["音视频"]},{"title":"实现FFmpeg的解复用","url":"/2024/12/06/实现FFmpeg的解复用/","content":"\n## 程序步骤\n\n### 1. 处理参数\n```cpp\nchar* src;  //输入数据\nchar* dst;  //输出数据\n\nint *stream_map = NULL;\n\nAVFormatContext *pFmtCtx = NULL;\nAVFormatContext *oFmtCtx = NULL;\n\nAVPacket pkt;\n\nav_log_set_level(AV_LOG_DEBUG);\nif(argc < 3){ //argv[0] 就是这个extra_audio名字\n    av_log(NULL, AV_LOG_INFO, \"arguments must be more than 3!\");\n    exit(-1);\n}\n\nsrc = argv[1];\ndst = argv[2];\n```\n\n---\n\n### 2. 配置输入上下文，打开多媒体文件\n通过 avformat_open_input 打开输入的多媒体文件，并初始化输入上下文：\n```cpp\nif((ret = avformat_open_input(&pFmtCtx, src, NULL, NULL)) < 0){\n    av_log(NULL, AV_LOG_ERROR, \"%s\\n\", av_err2str(ret));\n    exit(-1);\n}\n```\n\n---\n\n### 3. 配置输出上下文\n通过 avformat_alloc_output_context2 为目标文件创建输出上下文：\n```cpp\navformat_alloc_output_context2(&oFmtCtx, NULL, NULL, dst);\nif(!oFmtCtx){\n    av_log(NULL ,AV_LOG_ERROR, \"NO MEMORY\");\n    goto __ERROR;\n}\n```\n\n---\n\n### 4. 打开输出文件\n通过 avio_open2 打开目标文件，用于写入处理后的多媒体数据：\n```cpp\nret = avio_open2(&oFmtCtx->pb, dst, AVIO_FLAG_WRITE, NULL, NULL);\nif(ret < 0){\n    av_log(oFmtCtx, AV_LOG_ERROR, \"%s\", av_err2str(ret));\n    goto __ERROR;\n}\n```\n\n---\n\n### 5. 在输出上下文创建流\n遍历输入文件的所有流，根据需要创建对应的输出流，同时初始化 stream_map：\n```cpp\nstream_map = av_calloc(pFmtCtx->nb_streams, sizeof(int));\nif(!stream_map){\n    av_log(NULL ,AV_LOG_ERROR, \"NO MEMORY\");\n    goto __ERROR;\n}\n\nfor(int i = 0; i < pFmtCtx->nb_streams; i++){\n    AVStream *outStream = NULL;\n    AVStream *inStream = pFmtCtx->streams[i];\n    AVCodecParameters *inCodecPar = inStream->codecpar;\n    if(inCodecPar->codec_type != AVMEDIA_TYPE_AUDIO && inCodecPar->codec_type != AVMEDIA_TYPE_VIDEO && inCodecPar->codec_type != AVMEDIA_TYPE_SUBTITLE){\n        stream_map[i] = -1;\n        continue;\n    }\n    stream_map[i] = stream_idx++;\n    outStream = avformat_new_stream(oFmtCtx, NULL);\n    if(!outStream){\n        av_log(NULL ,AV_LOG_ERROR, \"NO MEMORY\");\n        goto __ERROR;\n    }\n\n    //设置输出视频参数\n    avcodec_parameters_copy(outStream->codecpar, inStream->codecpar);\n    outStream->codecpar->codec_tag = 0; //自动设置编解码器，对于输出流在这里是自动设置编码器\n}\n```\n\n---\n\n### 6. 写多媒体文件头到目的文件\n```cpp\nret = avformat_write_header(oFmtCtx, NULL);\nif(ret < 0){\n    av_log(oFmtCtx, AV_LOG_ERROR, \"%s\", av_err2str(ret));\n    goto __ERROR;\n}\n```\n\n---\n\n### 7. 从源多媒体文件中读取数据到目的文件中\n通过 av_read_frame 读取输入文件的帧数据，并通过 av_interleaved_write_frame 将处理后的数据写入目标文件：\n```cpp\nwhile(av_read_frame(pFmtCtx, &pkt) >= 0){\n    AVStream *inStream, *outStream;\n    if(stream_map[pkt.stream_index] < 0){\n        av_packet_unref(&pkt);\n        continue;\n    }\n    inStream = pFmtCtx->streams[pkt.stream_index];\n    pkt.stream_index = stream_map[pkt.stream_index];\n    outStream = oFmtCtx->streams[pkt.stream_index];\n    av_packet_rescale_ts(&pkt, inStream->time_base, outStream->time_base);\n    //查看ffmpeg源码可以知道在写入包之前会有一个guess_pkt_duration(s, st, pkt),这个函数会自动设置duration\n    //pkt.pos = -1; //用于索引标识，在特殊的场景下可以使用，比如合并多个文件。\n    av_interleaved_write_frame(oFmtCtx, &pkt);\n    av_packet_unref(&pkt);\n}\n```\n\n---\n\n### 8. 写多媒体文件尾到文件中\n```cpp\nav_write_trailer(oFmtCtx);\n```\n\n---\n\n### 9. 将申请的资源释放掉\n```cpp\n__ERROR:\nif(pFmtCtx){\n    avformat_close_input(&pFmtCtx);\n    pFmtCtx = NULL;\n}\nif(oFmtCtx->pb){\n    avio_close(oFmtCtx->pb);\n}\nif(oFmtCtx){\n    avformat_free_context(oFmtCtx);\n    oFmtCtx = NULL;\n}\nif(stream_map){\n    av_free(stream_map);\n}\n```","tags":["FFmpeg"],"categories":["音视频"]},{"title":"对ffmpeg中时间基与时间戳的理解","url":"/2024/12/05/对ffmpeg中时间基与时间戳的理解/","content":"\n### 时间基与时间戳的基本概念\n在 FFmpeg 中，时间基(time_base)是时间戳(timestamp)的单位，时间戳值乘以时间基，可以得到实际的时刻值(以秒等为单位)。\n\n### pts和dts\n这里我将pts和dts简单的理解为`pts是播放时间`，`dts是解码时间`\n视频帧的解码时间和播放时间可能不同，尤其是当视频帧包含B帧时。\n对于I帧，解码时间和播放时间相同。而B帧和P帧的dts和pts就会不一样。\n音频帧的解码时间和播放时间绝大多数的情况下是相同的，因为音频帧按顺序解码并播放。\n\n### 视频流和音频流pts和dts的设置(基于裸流)\n#### 视频流\n`视频按帧播放`，所以解码后的原始视频帧时间基为 1/帧率。  \n对于一般的视频流，时间基的 `num` 和 `den` 为 1 和 90000，时间基就是 1/90000。\n这样计算视频的时间戳就是 `index / 25 * 90000`，`index`是帧标识，表示第几帧。\n即pts就是`index / 25 * 90000`\n而dts我发现不设置也可以(但具体原因我也不知道，有知道的还请一定指点一二，拜托了`(ಥ _ ಥ)`)\n其中，`25` 是视频的帧率（frames per second，fps），即每秒钟的帧数。  \n通过将时间戳乘以时间基，可以得到实际的播放时间。\n换算成时间基的目的，主要是为了统一不视频流的时间单位，就像统一国际单位一样，不同帧率的视频时间戳都有统一的单位。\n这样的换算也能进一步提高精度。\n\n#### 音频流\n`音频按采样点播放`，所以解码后的原始音频帧时间基为 1/采样率。  \n这样音频的时间戳设置就是采样数，时间基就是采样率的倒数。即，采样率越高，每个采样点的时间基就越小。\n\n### 重点\n理解这些最需要记住的一点是：`时间基乘以时间戳得到的是实际时间`。","tags":["FFmpeg"],"categories":["音视频"]},{"title":"解决git操作push失败的问题","url":"/2024/12/04/解决git操作push失败的问题/","content":"\n从网上找到了一个教程，解决了一个烦人的问题\n\n### 设置代理\n```bash\ngit config --global http.proxy http://127.0.0.1:7890  \ngit config --global https.proxy http://127.0.0.1:7890\n```\n\n### 取消和查看代理\n\n#### 取消代理\n```bash\ngit config --global --unset http.proxy  \ngit config --global --unset https.proxy\n```\n\n#### 查看代理\n```bash\ngit config --global --get http.proxy  \ngit config --global --get https.proxy  \ngit config --list\n```\n\n原文链接：[https://blog.csdn.net/Naylor_5/article/details/135648311](https://blog.csdn.net/Naylor_5/article/details/135648311)","tags":["解决","git"],"categories":["杂项"]},{"title":"利用rtmp对本地flv文件进行推流","url":"/2024/12/03/利用rtmp对本地flv文件进行推流/","content":"\n### 实现了从一个 FLV 文件读取音视频数据，并通过 RTMP 协议进行推流。核心步骤包括：\n\n#### **FLV 文件头解析 (openfile)**  \n   - **功能**: 打开并读取 FLV 文件，跳过文件头（9 字节）。  \n   - **步骤**:\n     - 打开指定的 FLV 文件。\n     - 检查文件是否成功打开。如果打开失败，则返回 `nullptr`。\n     - 跳过文件头的 9 字节，定位到数据部分。\n\n```cpp\nstatic QFile *openfile(char *flv_name) {\n    QFile *file = new QFile(flv_name);\n    if(!file->open(QFile::ReadOnly)){\n        qDebug() << \"文件打开失败!\";\n        return nullptr;\n    }\n    file->seek(9); //跳过 9字节 header\n    return file;\n}\n```\n\n#### **RTMP 连接 (connect_rtmp_server)**  \n   - **功能**: 初始化 RTMP 连接到 RTMP 服务器。  \n   - **步骤**:\n     - 检查 RTMP 地址是否为空。\n     - 创建并初始化 RTMP 对象。\n     - 设置 RTMP 服务器地址和连接超时时间。\n     - 进行连接，并设置为推流模式（调用 `RTMP_EnableWrite`）。\n     - 创建并连接流。\n\n```cpp\nstatic RTMP *connect_rtmp_server(char *rtmpaddr) {\n    if(rtmpaddr == NULL){\n        qDebug() << \"连接的RTMP地址为空!\";\n        return NULL;\n    }\n    RTMP *rtmp = nullptr;\n    rtmp = RTMP_Alloc();\n    RTMP_Init(rtmp);\n    if(!rtmp){\n        qDebug() << \"Failed to alloc RTMP object!\";\n        goto __ERROR;\n    }\n\n    RTMP_SetupURL(rtmp, rtmpaddr);\n    rtmp->Link.timeout = 10;\n\n    if(!RTMP_Connect(rtmp, NULL)){\n        qDebug() << \"Failed to connect RTMP Server!\";\n        goto __ERROR;\n    }\n\n    RTMP_EnableWrite(rtmp);\n    RTMP_ConnectStream(rtmp, 0);\n\n    return rtmp;\n__ERROR:\n    if(rtmp){\n        RTMP_Close(rtmp);\n        RTMP_Free(rtmp);\n    }\n    return NULL;\n}\n```\n\n#### **RTMP 数据包分配 (alloc_packet)**  \n   - **功能**: 为 RTMP 数据包分配内存并初始化。  \n   - **步骤**:\n     - 分配 `RTMPPacket` 内存空间。\n     - 分配 64KB 内存空间用于数据包。\n     - 重置数据包并设置初始值。\n\n```cpp\nstatic RTMPPacket *alloc_packet() {\n    RTMPPacket *packet = NULL;\n    packet = (RTMPPacket *)malloc(sizeof(RTMPPacket));\n    if(!packet){\n        qDebug() << \"Failed to alloc RTMPPacket!\";\n        return NULL;\n    }\n\n    RTMPPacket_Alloc(packet, 64 * 1024);\n    RTMPPacket_Reset(packet);\n\n    packet->m_hasAbsTimestamp = 0;\n    packet->m_nChannel = 0x4;\n\n    return packet;\n}\n```\n\n#### **读取 FLV 数据并填充 RTMP 数据包 (read_data)**  \n   - **功能**: 逐个读取 FLV 文件中的数据块并填充 RTMP 数据包。  \n   - **步骤**:\n     - 跳过前 4 字节的 `pre-tag size`。\n     - 读取 tag type、tag data size、时间戳和流 ID。\n     - 读取实际的 tag 数据体，并将其填充到 `RTMPPacket` 中。\n\n```cpp\nstatic int read_data(QFile *file, RTMPPacket **packet) {\n    if (!file || !packet) {\n        qDebug() << \"Error: Invalid arguments.\";\n        return 0;\n    }\n\n    int ret = 0;\n    QByteArray tt, tag_data_size, ts, streamid;\n\n    file->read(4);  // 跳过 pre-tag size\n    tt = file->read(1);  // 读取 tag type\n    tag_data_size = file->read(3);  // 读取 tag data size\n\n    unsigned int tagDataSize = ((unsigned char)tag_data_size[0] << 16)\n                               | ((unsigned char)tag_data_size[1] << 8)\n                               | (unsigned char)tag_data_size[2];\n\n    if (file->bytesAvailable() < tagDataSize) {\n        qDebug() << \"Error: Insufficient data for tag body.\";\n        return 0;\n    }\n\n    ts = file->read(4);  // 读取时间戳\n    unsigned int timestamp = ((unsigned char)ts[0] << 16)\n                             | ((unsigned char)ts[1] << 8)\n                             | ((unsigned char)ts[2]);\n\n    streamid = file->read(3);  // 读取流 ID\n    QByteArray bodyData = file->read(tagDataSize);  // 读取 tag body\n\n    (*packet)->m_packetType = static_cast<unsigned char>(tt[0]);\n    std::memcpy((*packet)->m_body, bodyData.constData(), tagDataSize);\n    (*packet)->m_headerType = RTMP_PACKET_SIZE_LARGE;\n    (*packet)->m_nTimeStamp = timestamp;\n    (*packet)->m_nBodySize = tagDataSize;\n\n    qDebug() << \"tt:\" << static_cast<unsigned char>(tt[0])\n             << \"ts:\" << timestamp\n             << \"datasize:\" << tagDataSize;\n\n    ret = 1; // 成功\n    return ret;\n}\n```\n\n#### **推流到 RTMP 服务器 (send_data)**  \n   - **功能**: 从 FLV 文件中读取数据并通过 RTMP 协议发送。  \n   - **步骤**:\n     - 创建 `RTMPPacket` 对象。\n     - 逐个读取 FLV 数据包并发送到 RTMP 服务器。\n     - 如果连接断开，则中断发送。\n     - 使用 `RTMP_SendPacket` 发送数据包到 RTMP 服务器。\n\n```cpp\nstatic void send_data(QFile *file, RTMP *rtmp) {\n    RTMPPacket *packet = alloc_packet();\n\n    while(read_data(file, &packet)) {\n        if(!RTMP_IsConnected(rtmp)) {\n            qDebug() << \"Disconnect...\";\n            break;\n        }\n\n        if(!RTMP_SendPacket(rtmp, packet, 0)) {\n            qDebug() << \"Failed to send packet!\";\n        }\n    }\n\n    qDebug() << \"发送结束\";\n}\n```\n\n#### **RTMP 推流整体的封装 (publish_stream)**  \n   - **功能**: 完成 FLV 文件读取、RTMP 连接、数据发送等操作。  \n   - **步骤**:\n     - 打开 FLV 文件并连接到 RTMP 服务器。\n     - 将 FLV 文件中的音视频数据逐个发送到 RTMP 服务器。\n     - 完成推流后，关闭文件。\n\n```cpp\nvoid publish_stream() {\n    char *rtmpaddr = (char *)\"rtmp://127.0.0.1:1935/live/stream01\";\n    QFile *file = openfile((char *)\"output.flv\");\n    if(!file) {\n        return;\n    }\n\n    RTMP *rtmp = connect_rtmp_server(rtmpaddr);\n    if(!rtmp) {\n        return;\n    }\n\n    send_data(file, rtmp);\n    file->close();\n}\n```\n\n#### **线程运行入口 (run)**  \n   - **功能**: 在线程中执行发布流的操作。  \n   - **步骤**:\n     - 初始化 Windows 套接字库 (`WSAStartup`)。\n     - 调用 `publish_stream` 完成推流任务。\n     - 调用 `WSACleanup` 关闭套接字库。\n\n```cpp\nvoid myRtmp::run() {\n    WSADATA wsaData;\n    int result = WSAStartup(MAKEWORD(2, 2), &wsaData);\n    if (result != 0) {\n        qDebug() << \"WSAStartup failed: \" << result;\n        return;\n    }\n    publish_stream();\n    WSACleanup();\n}\n```\n\n### 主要结构和功能：\n- **RTMP 流传输**: 使用 RTMP 协议推送 FLV 文件数据到服务器。\n- **文件处理**: 从 FLV 文件中提取音视频数据并填充到 RTMP 数据包中。\n- **多线程支持**: 使用 `QThread` 在独立线程中执行推流任务。","tags":["FLV","推流","RTMP"],"categories":["音视频"]},{"title":"①实现电脑音频和视频的录制并封装成mp4","url":"/2024/12/02/①实现电脑音频和视频的录制并封装成mp4/","content":"\n### 头文件部分\n\n内容比较少，我就全部放这了, 自己看吧~\n\n```cpp\n#include \"AudioRecorder.h\"\n#include \"videoRecorder.h\"\n\n#include <QMainWindow>\n\nQT_BEGIN_NAMESPACE\nnamespace Ui {\nclass MainWindow;\n}\nQT_END_NAMESPACE\n\nclass MainWindow : public QMainWindow\n{\n    Q_OBJECT\n\npublic:\n    MainWindow(QWidget *parent = nullptr);\n    ~MainWindow();\n\n    int open_device(AVFormatContext **fmt_ctx);\n    int Main_av_interleaved_write_frame(AVFormatContext *s, AVPacket *pkt);\n    int Main_avformat_write_header(AVFormatContext *s, AVDictionary **options);\n\nprivate slots:\n    void on_pushButton_clicked();\n\nprivate:\n    Ui::MainWindow *ui;\n    AudioRecorder *aread;\n    VideoRecorder *vread;\n    // create file 设置文件操作对象用于存储消息\n    const char* outFilename = \"output.mp4\";\n    AVFormatContext *ofmt_ctx = nullptr; // 输出上下文\n    QReadWriteLock lock; // 线程安全，读锁 不允许在读取时进行修改; 写锁 不允许在写的时候进行读写 注意读写锁不能包含在同一个作用域里\n};\n```\n\n### 源文件部分\n\n```cpp\n#include \"./ui_mainwindow.h\"\n#include \"mainwindow.h\"\n\nMainWindow::MainWindow(QWidget *parent)\n    : QMainWindow(parent)\n    , ui(new Ui::MainWindow)\n{\n    ui->setupUi(this);\n    aread = new AudioRecorder(this, &ofmt_ctx);\n    vread = new VideoRecorder(this, &ofmt_ctx);\n}\n\nMainWindow::~MainWindow()\n{\n    delete ui;\n}\n\nint MainWindow::Main_av_interleaved_write_frame(AVFormatContext *s, AVPacket *pkt)\n{\n    QWriteLocker locker(&lock); // 写锁\n    return av_interleaved_write_frame(s, pkt);\n}\n\nint stream_nb = 0;\n\nint MainWindow::Main_avformat_write_header(AVFormatContext *s, AVDictionary **options)\n{\n    // 写头文件\n    stream_nb += 1;\n    if (stream_nb == 1) {\n        qDebug() << \"等待下一个流\";\n        while (1) {\n            if (stream_nb == 2) return 1;\n        }\n    };\n    return avformat_write_header(ofmt_ctx, NULL);\n};\n\nvoid MainWindow::on_pushButton_clicked()\n{\n    if (!vread->flage) {\n        // 配置输出上下文\n        avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, outFilename);\n        if (!ofmt_ctx) {\n            qDebug() << \"创建输出上下文失败!\";\n            return;\n        }\n        // 打开输出\n        if (!(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) {\n            // 2.3 创建并初始化一个AVIOContext, 用以访问URL（outFilename）指定的资源\n            if (avio_open(&ofmt_ctx->pb, outFilename, AVIO_FLAG_WRITE) < 0) {\n                qDebug() << \"can't open output URL: %s\\n\" << outFilename;\n                return;\n            }\n        }\n        stream_nb = 0;\n        aread->flage = true;\n        vread->flage = true;\n        aread->start();\n        vread->start();\n        ui->pushButton->setText(\"停止\");\n    } else {\n        aread->flage = false;\n        vread->flage = false;\n        aread->wait();\n        vread->wait();\n        av_write_trailer(ofmt_ctx);\n        /* close output */\n        if (ofmt_ctx && !(ofmt_ctx->oformat->flags & AVFMT_NOFILE)) {\n            avio_closep(&ofmt_ctx->pb);\n        }\n        avformat_free_context(ofmt_ctx);\n        ui->pushButton->setText(\"开始\");\n    }\n}\n```\n---\n\n### **总结**\n通过多线程, 调用FFmpeg的API，音频采集部分利用FIFO缓冲区存储音频数据，视频采集部分直接存放于pFrameYUV，音频经过解码、编码成AAC后进行写入，视频经过解码、编码成H264后进行写入\n第一次写，感觉写的很乱，有很多不足的地方。 有不好的地方可以帮忙指出来 让我偷偷懒吧~~~","tags":["FFmpeg","多线程","QT","C++"],"categories":["音视频"]},{"title":"②实现电脑音频和视频的录制并封装成mp4","url":"/2024/12/02/②实现电脑音频和视频的录制并封装成mp4/","content":"\n\n## VideoRecorder 类详解\n\n### 头文件部分\n\n```cpp\n// 构造函数\nVideoRecorder(QObject* parent = nullptr, AVFormatContext **ofmt_ctx = nullptr);\n\n// 变量\nvoid run() override;\nbool flage = false;\nAVFormatContext *fmt_ctx = NULL; // 输入上下文\nAVFormatContext **ofmt_ctx = NULL; // 输出上下文\nQObject *parent = NULL;\n```\n\n### 源文件部分\n\n采集视频的具体步骤分为9步，涉及以下变量：\n\n```cpp\nAVPacket pkt; // 音频包\nAVPacket *H264pkt = av_packet_alloc();\nAVCodecContext *dec_ctx = NULL;\nAVCodecContext *H264_Codec_ctx = NULL;\nAVFrame *frame = av_frame_alloc();\nAVPacket *newpkt = av_packet_alloc();\nconst char* outFilename = \"output.mp4\"; // 输出文件\n```\n\n#### 步骤 1：打开视频设备\n\n```cpp\nopen_device(&fmt_ctx)\n```\n\n#### 步骤 2：打开视频解码器\n\n```cpp\nopen_video_decoder(fmt_ctx, &dec_ctx)\n```\n\n#### 步骤 3：配置视频编码器并打开\n\n```cpp\nopen_video_encoder(&H264_Codec_ctx, dec_ctx)\n```\n\n#### 步骤 4：创建输出流\n\n```cpp\n// 创建输出流\nAVStream *outStream = avformat_new_stream(*ofmt_ctx, NULL);\navcodec_parameters_from_context(outStream->codecpar, H264_Codec_ctx); // 同样这里也是输出视频流的相关信息\n```\n\n#### 步骤 5：配置图像重采样上下文\n\n```cpp\n// 图像格式转换上下文\nSwsContext* pImgConvertCtx = sws_getContext(dec_ctx->width, dec_ctx->height,\n                                            dec_ctx->pix_fmt, dec_ctx->width, dec_ctx->height,\n                                            AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL);\n```\n\n#### 步骤 6：写头文件\n\n```cpp\n// 写头文件\nCallbackFun writehead = std::bind(&MainWindow::Main_avformat_write_header, static_cast<MainWindow *>(parent), *ofmt_ctx, nullptr);\nwritehead();\n```\n\n#### 步骤 7：配置视频数据存储容器 pFrameYUV\n\n```cpp\n// 初始化帧编号\nint64_t pts = 0;\n\nAVFrame *pFrameYUV = av_frame_alloc();\npFrameYUV->format = AV_PIX_FMT_YUV420P;\npFrameYUV->width = dec_ctx->width;\npFrameYUV->height = dec_ctx->height;\nav_frame_get_buffer(pFrameYUV, 0);\n```\n\n#### 步骤 8：读取数据并写入文件\n\n```cpp\n// 使用 av_read_frame() 读取音频数据，并写入文件\n```\n\n#### 步骤 9：释放分配的空间\n\n```cpp\n// 释放缓冲区的数据和相关上下文\nif(frame) av_frame_free(&frame);\nif(pFrameYUV) av_frame_free(&pFrameYUV);\nif(newpkt) av_packet_free(&newpkt);\n\nsws_freeContext(pImgConvertCtx);\n\n// 关闭设备并释放 fmt_ctx 资源\navformat_close_input(&fmt_ctx);\n```\n\n### open_device(&fmt_ctx)的具体实现\n\n```cpp\nint VideoRecorder::open_device(AVFormatContext **fmt_ctx){\n    const AVInputFormat * iformat = NULL;  // 音视频捕获格式\n    AVDictionary *options = NULL;\n    QString devicename;  // 音视频设备名称\n\n    // 注册音视频设备\n    avdevice_register_all();\n\n    // 获取音视频格式\n    iformat = av_find_input_format(\"dshow\");\n\n    // 获取当前音视频设备的描述名称\n    // if (avdevice_list_input_sources(iformat, NULL, NULL, &device_list) >= 0) {\n    //     for (int i = 0; i < device_list->nb_devices; i++) {\n    //         qDebug() << \"Device: \" << device_list->devices[i]->device_description << Qt::endl;\n    //         if(i == 1) devicename = \"audio=\" + QString(device_list->devices[i]->device_description);\n    //     }\n    // }\n\n    // video=USB2.0 HD UVC WebCam 摄像头设备名称 || audio=麦克风阵列 (Realtek(R) Audio) 麦克风设备名称\n    // 可以用冒号video=\"Camera\":audio=\"Microphone\" 进行同时使用\n    devicename = \"video=USB2.0 HD UVC WebCam\";\n\n    av_dict_set(&options, \"video_size\", \"1280x720\", 0);\n    // av_dict_set(&options, \"pixel_fmt\", \"yuv420p\", 0);\n    av_dict_set(&options, \"framerate\", \"30\", 0);\n\n    // 打开视频流设备 并初始化上下文\n    if(avformat_open_input(fmt_ctx, devicename.toUtf8(), iformat, &options) < 0){\n        // av_strerror(ret, errors, 1024);\n        // qDebug() << \"Failed to open audio device: \" << ret << errors;\n        return 0;\n    };\n    return 1;\n}\n```\n\n### open_video_decoder(fmt_ctx, &dec_ctx)的具体实现\n\n```cpp\nint VideoRecorder::open_video_decoder(AVFormatContext *fmt_ctx, AVCodecContext **dec_ctx){\n    const AVCodec *dec;\n    int ret;\n\n    if((ret = avformat_find_stream_info(fmt_ctx, NULL)) < 0){\n        qDebug() << \"无法获取流信息! \" << ret;\n        return 0;\n    };\n\n    // 选择视频流\n    ret = av_find_best_stream(fmt_ctx, AVMEDIA_TYPE_VIDEO, -1, -1, &dec, 0);\n    if(ret < 0){\n        qDebug() << \"无法找到视频流 \" << ret;\n        return 0;\n    }\n\n    int video_stream_index = ret;\n\n    // 创建解码器上下文,并初始化上下文\n    *dec_ctx = avcodec_alloc_context3(dec);\n    if(!*dec_ctx) return 0;\n    avcodec_parameters_to_context(*dec_ctx, fmt_ctx->streams[video_stream_index]->codecpar);\n\n    // 打开解码器\n    if((ret = avcodec_open2(*dec_ctx, dec, nullptr)) < 0){\n        qDebug() << \"解码器打开失败! \" << ret;\n        return 0;\n    };\n\n    return 1;\n}\n```\n\n### open_video_encoder(&H264_Codec_ctx, dec_ctx)的具体实现\n\n```cpp\nint VideoRecorder::open_video_encoder(AVCodecContext **H264_Codec_ctx, AVCodecContext *dec_ctx){\n    AVDictionary *options = NULL;\n\n    av_dict_set(&options, \"preset\", \"superfast\", 0);\n    av_dict_set(&options, \"tune\", \"zerolatency\", 0);  // 实现实时编码\n\n    const AVCodec *H264_Codec = avcodec_find_encoder(AV_CODEC_ID_H264);\n    if(!H264_Codec){\n        qDebug() << \"未找到H264编码器!\";\n        return 0;\n    }\n\n    *H264_Codec_ctx = avcodec_alloc_context3(H264_Codec);\n\n    // 设置编码器格式\n    (*H264_Codec_ctx)->codec_id = AV_CODEC_ID_H264; // option\n    (*H264_Codec_ctx)->codec_type = AVMEDIA_TYPE_VIDEO; // option\n    (*H264_Codec_ctx)->pix_fmt = AV_PIX_FMT_YUV420P;\n\n    // 设置SPS/PPS\n    (*H264_Codec_ctx)->profile = FF_PROFILE_H264_HIGH_444; // 压缩等级\n    (*H264_Codec_ctx)->level = 50; // 质量等级为5.0\n\n    // 设置分辨率\n    (*H264_Codec_ctx)->width = dec_ctx->width;\n    (*H264_Codec_ctx)->height = dec_ctx->height;\n\n    // 设置GOP\n    (*H264_Codec_ctx)->gop_size = 250;\n    (*H264_Codec_ctx)->keyint_min = 25; // 最小插入I帧的帧数 在网络丢帧时用于恢复 option\n\n    // 设置B帧数据可减小码流\n    (*H264_Codec_ctx)->max_b_frames = 3; // 可连续的最大B帧数量 option\n    (*H264_Codec_ctx)->has_b_frames = 1; // 指示编码器是否生成 B 帧以及解码器是否需要处理 B 帧 option\n\n    // 设置参考帧数量可提升还原度\n    (*H264_Codec_ctx)->refs = 3; // option\n\n    // 设置帧率\n    (*H264_Codec_ctx)->time_base.num = 1\n\n;\n    (*H264_Codec_ctx)->time_base.den = 30;\n\n    // 设置比特率\n    (*H264_Codec_ctx)->bit_rate = 3420000;\n\n    // 打开编码器\n    if((avcodec_open2(*H264_Codec_ctx, H264_Codec, &options)) < 0){\n        qDebug() << \"H264 编码器打开失败!\";\n        return 0;\n    }\n\n    return 1;\n}\n```\n\n### 数据的读取与写入的具体实现\n\n```cpp\nwhile(flage && av_read_frame(fmt_ctx, &pkt) == 0){\n    int ret = avcodec_send_packet(dec_ctx, &pkt);\n    if(ret < 0) return;\n\n    while(ret >= 0){\n        ret = avcodec_receive_frame(dec_ctx, frame);\n        if(ret < 0){\n            if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break;\n            return;\n        }\n        sws_scale(pImgConvertCtx, (const unsigned char* const*)frame->data, frame->linesize, 0, dec_ctx->height, pFrameYUV->data, pFrameYUV->linesize);\n\n        ret = avcodec_send_frame(H264_Codec_ctx, pFrameYUV);\n        if(ret < 0) return;\n\n        while(ret >= 0){\n            ret = avcodec_receive_packet(H264_Codec_ctx, H264pkt);\n            if(ret < 0){\n                if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break;\n                return;\n            }\n\n            // 编码后PTS、DTS赋值\n            H264pkt->pts = H264pkt->dts = pts * (*ofmt_ctx)->streams[video_stream_index]->time_base.den / (*ofmt_ctx)->streams[video_stream_index]->time_base.num / 30;\n            H264pkt->stream_index = video_stream_index;\n\n            CallbackFun write = std::bind(&MainWindow::Main_av_interleaved_write_frame, static_cast<MainWindow *>(parent), *ofmt_ctx, H264pkt);\n            write();\n\n            pts++;\n            av_packet_unref(H264pkt);\n        }\n        av_frame_unref(frame);\n    }\n    av_packet_unref(&pkt); // Free the packet data after each read\n}\n```","tags":["FFmpeg","QT","H264"],"categories":["音视频"]},{"title":"③实现电脑音频和视频的录制并封装成mp4","url":"/2024/12/01/③实现电脑音频和视频的录制并封装成mp4/","content":"\n### **音视频录制的多线程实现**\n\n#### **主要类**\n- **AudioRecorder**：音频采集线程\n- **VideoRecorder**：视频采集线程\n- **MainWindow**：控制音频和视频的采集\n\n---\n\n### **AudioRecorder 类详解**\n\n#### **头文件部分**\n- **构造函数**：\n  ```cpp\n  AudioRecorder(QObject *parent = nullptr, AVFormatContext **ofmt_ctx = nullptr);\n  ```\n\n- **主要变量**：\n  - `bool flage = false;`  // 用于控制采集的开始与结束\n  - `AVFormatContext *fmt_ctx = NULL;`  // 输入上下文\n  - `AVFormatContext **ofmt_ctx = NULL;`  // 输出上下文\n  - `QObject *parent = NULL;`  // 用于回调函数\n\n\n#### 源文件部分\n\n采集音频的具体步骤可以分为10个步骤，使用到的变量有：\n\n- `AVPacket pkt;`：音频包\n- `int nb_samples = 22050;`：每帧样本数\n- `SwrContext *swr_ctx = nullptr;`：重采样上下文\n- `uint8_t **src_data = nullptr; uint8_t **dst_data = nullptr;`：音频数据缓冲区\n- `AVCodecContext *codec_ctx = nullptr;`：编码上下文\n- `AVCodecContext *dec_ctx = nullptr;`：解码上下文\n- `AVFrame *frame = av_frame_alloc();`：音频帧\n- `AVAudioFifo* fifo = nullptr;`：音频FIFO缓冲区\n- `const char* outFilename = \"output.mp4\";`：输出文件名\n\n#### **音频采集步骤**\n1. **打开设备**  \n   ```cpp\n   open_device(&fmt_ctx);\n   ```\n\n2. **查找并打开音频解码器**  \n   ```cpp\n   open_Audio_decoder(fmt_ctx, &dec_ctx);\n   ```\n\n3. **打开AAC编码器**  \n   ```cpp\n   open_encoder(&codec_ctx);\n   ```\n\n4. **建立FIFO缓冲区**  \n   ```cpp\n   fifo = init_audio_fifo(codec_ctx->sample_fmt, codec->ch_layout.nb_channels);\n   ```\n\n5. **配置重采样上下文（如果需要，根据编码器的输入要求）**  \n   ```cpp\n   init_resampler(codec_ctx, &swr_ctx, &src_data, &dst_data);\n   ```\n\n6. **创建输出流**  \n   ```cpp\n   AVStream *outStream = avformat_new_stream(*ofmt_ctx, NULL);\n   ```\n\n7. **写头文件（通过回调方式）**\n\n为了确保音频流和视频流都创建好后再写入头文件，我采用了回调函数的方式。FFmpeg在写入头文件时要求所有流都已经创建好，因此我们通过回调机制确保音频流和视频流的创建顺序。\n\n#### **回调函数的实现**\n\n在代码中，我们定义了一个回调函数别名，并将其与`MainWindow`的`Main_avformat_write_header`方法绑定，确保在合适的时机调用该函数来写入头文件。\n\n```cpp\n// 定义回调函数别名，绑定到 MainWindow::Main_avformat_write_header 方法\nCallbackFun writehead = std::bind(&MainWindow::Main_avformat_write_header, \n                                   static_cast<MainWindow *>(parent), \n                                   *ofmt_ctx, nullptr);\n\n// 调用回调函数执行写入头文件操作\nwritehead();\n```\n\n#### **解释**\n\n- `std::bind`：该方法将`MainWindow::Main_avformat_write_header`函数与`parent`（`MainWindow`）对象绑定，并传入必要的参数。\n- 通过回调的方式，只有在音频和视频流都准备好后才会调用`Main_avformat_write_header`方法进行头文件写入，确保FFmpeg正确地写入头文件。\n\n#### **为什么要使用回调**\n\nFFmpeg的`avformat_write_header()`函数要求在写入文件头之前，所有流都必须被创建并初始化。因此，在创建音频和视频流后，我们通过回调函数来确保头文件的写入时机是正确的。\n\n\n8. **配置解码后数据的frame容器及音频帧**\n\n```cpp\n    //获取frame_size\n    const int frame_size = codec_ctx->frame_size;\n    // 初始化帧编号\n    int64_t pts = 0;\n\n    frame->nb_samples = dec_ctx->frame_size;\n    frame->sample_rate = dec_ctx->sample_rate;\n    frame->ch_layout = dec_ctx->ch_layout;\n    frame->format = dec_ctx->sample_fmt;\n```\n\n9. **用 `av_read_frame()` 读取音频数据并写入文件**\n\n\n10. **释放分配的空间**\n\n```cpp\n    // 释放缓冲区的数据和相关上下文\n    if(src_data){\n        av_freep(&src_data[0]);\n        av_freep(&src_data);\n    }\n    if(dst_data){\n        av_freep(&dst_data[0]);\n        av_freep(&dst_data);\n    }\n    if(swr_ctx)swr_free(&swr_ctx);\n    if(frame)av_frame_free(&frame);\n    if(newpkt)av_packet_free(&newpkt);\n    if(fifo)av_audio_fifo_free(fifo);\n\n    // close device and Release fmt_ctx resources 关闭设备并释放上下文\n    avformat_close_input(&fmt_ctx);\n```\n\n---\n\n### **open_device(&fmt_ctx) 的具体实现**\n\n```cpp\nint AudioRecorder::open_device(AVFormatContext **fmt_ctx) {\n    const AVInputFormat * iformat = NULL;  // 音视频捕获格式\n    AVDictionary *options = NULL;\n    QString devicename;  // 音视频设备名称\n\n    avdevice_register_all();  // 注册音视频设备\n\n    // 获取音视频格式\n    iformat = av_find_input_format(\"dshow\");\n\n    // 获取当前音频设备的描述名称\n    devicename = \"audio=麦克风阵列 (Realtek(R) Audio)\";\n\n    if (avformat_open_input(fmt_ctx, devicename.toUtf8(), iformat, &options) < 0) {\n        return 0;  // 打开设备失败\n    }\n    return 1;  // 成功打开设备\n}\n```\n\n---\n\n### **open_Audio_decoder() 的具体实现**\n\n```cpp\nint AudioRecorder::open_Audio_decoder(AVFormatContext **fmt_ctx) {\n    const AVCodec *codec = avcodec_find_decoder(AV_CODEC_ID_AAC);\n    if (!codec) {\n        qDebug() << \"音频解码器未找到\";\n        return 0;  // 未找到解码器\n    }\n    \n    // 打开音频解码器\n    if (avcodec_open2(dec_ctx, codec, nullptr) < 0) {\n        qDebug() << \"无法打开解码器\";\n        return 0;\n    }\n    \n    return 1;  // 成功打开解码器\n}\n```\n\n---\n\n### **open_encoder() 的具体实现**\n\n```cpp\nint AudioRecorder::open_encoder(AVCodecContext **codec_ctx) {\n    AVDictionary *options = NULL;\n    const AVCodec *codec = avcodec_find_encoder_by_name(\"libfdk_aac\");\n    if (!codec) {\n        qDebug() << \"编码器 libfdk_aac 未找到\";\n        return 0;\n    }\n\n    // 分配编码器上下文\n    *codec_ctx = avcodec_alloc_context3(codec);\n    if (!*codec_ctx) {\n        qDebug() << \"编码器上下文分配失败\";\n        return 0;\n    }\n\n    // 设置编码器参数\n    AVChannelLayout ch_layout;\n    av_channel_layout_default(&ch_layout, 2);  // 立体声布局\n    (*codec_ctx)->sample_rate = 48000;  // 采样率\n    (*codec_ctx)->sample_fmt = AV_SAMPLE_FMT_S16;  // 样本格式\n    (*codec_ctx)->bit_rate = 128000;  // 比特率\n    if (av_channel_layout_copy(&(*codec_ctx)->ch_layout, &ch_layout) < 0) {\n        qDebug() << \"设置通道布局失败\";\n        return 0;\n    }\n\n    // 打开编码器\n    if (avcodec_open2(*codec_ctx, codec, &options) < 0) {\n        qDebug() << \"编码器打开失败\";\n        return 0;\n    }\n\n    return 1;  // 成功打开编码器\n}\n```\n\n---\n\n### **FIFO缓冲区实现**\n\n```cpp\nAVAudioFifo* AudioRecorder::init_audio_fifo(AVSampleFormat sample_fmt, int channels) {\n    AVAudioFifo* fifo = av_audio_fifo_alloc(sample_fmt, channels, 1);\n    if (!fifo) {\n        qDebug() << \"无法分配FIFO缓冲区\";\n        return nullptr;\n    }\n    return fifo;\n}\n\nint AudioRecorder::write_to_fifo(AVAudioFifo* fifo, const uint8_t** data, int nb_samples) {\n    int ret = av_audio_fifo_write(fifo, (void**)data, nb_samples);\n    if (ret < nb_samples) {\n        qDebug() << \"写入FIFO失败\";\n        return -1;\n    }\n    return 0;  // 成功写入\n}\n\nint AudioRecorder::read_from_fifo(AVAudioFifo* fifo, uint8_t** data, int frame_size) {\n    int ret = av_audio_fifo_read(fifo, (void**)data, frame_size);\n    if (ret < frame_size) {\n        qDebug() << \"从FIFO读取失败\";\n        return -1;\n    }\n    return 0;  // 成功读取\n}\n```\n\n---\n\n### **init_resampler() 的具体实现**\n\n```cpp\nint AudioRecorder::init_resampler(AVCodecContext *codec_ctx, SwrContext **swr_ctx, uint8_t ***src_data, uint8_t ***dst_data) {\n    *swr_ctx = swr_alloc();\n    AVChannelLayout in_ch_layout;\n    av_channel_layout_default(&in_ch_layout, 2);  // 立体声布局\n\n    // 配置重采样上下文\n    swr_alloc_set_opts2(swr_ctx,\n                        &codec_ctx->ch_layout,          // 输出通道布局\n                        codec_ctx->sample_fmt,          // 输出采样格式\n                        codec_ctx->sample_rate,         // 输出采样率\n                        &in_ch_layout,                 // 输入通道布局\n                        AV_SAMPLE_FMT_S16,              // 输入采样格式\n                        48000,                         // 输入采样率\n                        0, nullptr);\n\n    if (!*swr_ctx || swr_init(*swr_ctx) < 0) {\n        qDebug() << \"重采样初始化失败\";\n        return 0;\n    }\n\n    // 创建输入输出缓冲区\n    av_samples_alloc_array_and_samples(src_data, nullptr, in_ch_layout.nb_channels, 22050, AV_SAMPLE_FMT_S16, 0);\n    av_samples_alloc_array_and_samples(dst_data, nullptr, 2, 22050, codec_ctx->sample_fmt, 0);\n    \n    return 1;  // 成功\n}\n```\n\n---\n\n### **音频数据读取与写入文件过程**\n\n```cpp\n //开始获取音频帧数据，并进行转换\n    while (flage && av_read_frame(fmt_ctx, &pkt) == 0) {\n        // qDebug() << \"pkt size: \" << pkt.size;\n        // qDebug() << \"pkt data: \" << pkt.data << Qt::endl;\n\n        //内存拷贝\n        //memcpy(src_data[0], pkt.data, pkt.size); //只使用第一个缓冲区\n        //如果有需要在写入文件之前进行重采样\n        //swr_convert(swr_ctx, dst_data, nb_samples, src_data, nb_samples);\n        int ret = avcodec_send_packet(dec_ctx, &pkt);\n        if(ret < 0){\n            qDebug() << \"向解码器发送数据包失败!\";\n            return;\n        }\n        while(ret >= 0){\n            ret = avcodec_receive_frame(dec_ctx, frame);\n            if(ret < 0){\n                if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                    break;\n                }\n                qDebug() << \"Error, decoding video frame\";\n                return;\n            }\n            // memcpy(src_data[0], frame->data, 22300);\n            // swr_convert(swr_ctx, dst_data, nb_samples, src_data, nb_samples);\n\n            //将样本数据写入 FIFO\n            write_to_fifo(fifo, (const uint8_t**)frame->data, nb_samples);\n            while (av_audio_fifo_size(fifo) >= frame_size) {\n                read_from_fifo(fifo, frame->data, frame_size);\n                frame->nb_samples = codec_ctx->frame_size;\n                frame->pts = pts;\n                ret = avcodec_send_frame(codec_ctx, frame);\n                if(ret < 0){\n                    qDebug() << \"向编码器发送数据包失败!\";\n                    return;\n                }\n                while(ret >= 0){\n                    ret = avcodec_receive_packet(codec_ctx, newpkt);\n                    if(ret < 0){\n                        if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){\n                            break;\n                        }\n                        qDebug() << \"Error, encoding video frame\";\n                        return;\n                    }\n                    newpkt->pts = newpkt->dts = pts;\n                    newpkt->stream_index = audio_stream_index;\n                    //av_interleaved_write_frame(*ofmt_ctx, newpkt);\n                    CallbackFun write = std::bind(&MainWindow::Main_av_interleaved_write_frame, static_cast<MainWindow *>(parent), *ofmt_ctx, newpkt);\n                    write();\n                    av_packet_unref(newpkt);\n                }\n                pts += frame_size;\n            }\n            av_frame_unref(frame);\n        }\n        av_packet_unref(&pkt); // Free the packet data after each read\n    }\n```","tags":["FFmpeg","QT","AAC"],"categories":["音视频"]},{"title":"SQL基础","url":"/2024/11/28/SQL基础/","content":"\n# 数据库约束与高级操作总结\n\n## 约束\n\n### 概念\n约束是作用于表中字段上的规则，用于限制存储在表中的数据。\n\n**目的**: 保证数据库中数据的正确性、有效性和完整性。\n\n**分类**:\n- **非空约束**: 限制字段数据不能为 NULL  \n  ```sql\n  NOT NULL\n  ```\n\n- **唯一约束**: 保证字段数据唯一、不重复  \n  ```sql\n  UNIQUE\n  ```\n\n- **主键约束**: 主键是行数据的唯一标识，要求非空且唯一  \n  ```sql\n  PRIMARY KEY\n  ```\n\n- **默认约束**: 保存数据时未指定字段值，则采用默认值  \n  ```sql\n  DEFAULT\n  ```\n\n- **检查约束** (8.0.16版本之后): 保证字段值满足条件  \n  ```sql\n  CHECK\n  ```\n\n- **外键约束**: 用来建立表间数据的关联，保证一致性与完整性  \n  ```sql\n  FOREIGN KEY\n  ```\n\n---\n\n### 外键约束\n\n#### 概念\n外键用于在两张表之间建立数据连接，保证数据的一致性和完整性。\n\n#### 语法\n- **添加外键**\n  ```sql\n  CREATE TABLE 表名 (\n    字段名 数据类型,\n    ...\n    [CONSTRAINT 外键名称] FOREIGN KEY (外键字段名) REFERENCES 主表 (主表列名)\n  );\n\n  ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段名) REFERENCES 主表 (主表列名);\n  ```\n\n- **删除外键**\n  ```sql\n  ALTER TABLE 表名 DROP FOREIGN KEY 外键名称;\n  ```\n\n#### 删除/更新行为\n- `NO ACTION`: 检查外键关联，不允许删除/更新。\n- `RESTRICT`: 与 `NO ACTION` 类似，不允许删除/更新。\n- `CASCADE`: 删除/更新父表记录时，同时删除/更新子表记录。\n- `SET NULL`: 删除父表记录时，将子表外键字段设置为 `NULL`。\n- `SET DEFAULT`: 设置子表外键字段为默认值（InnoDB 不支持）。\n\n**示例**:\n```sql\nALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段) REFERENCES 主表名 (主表字段名) ON UPDATE CASCADE ON DELETE CASCADE;\n```\n\n---\n\n## 多表查询\n\n### 概述\n多表查询指从多张表中获取数据。  \n需要避免无效的 **笛卡尔积**，即两张表中所有数据的组合。\n\n### 多表查询分类\n1. **连接查询**\n   - **内连接**: 获取两张表交集部分的数据。\n   - **左连接**: 获取左表所有数据及交集部分数据。\n   - **右连接**: 获取右表所有数据及交集部分数据。\n   - **自连接**: 表与自身的连接查询，必须使用表别名。\n\n2. **子查询**  \n   在 SQL 语句中嵌套 `SELECT` 语句，称为嵌套查询或子查询。\n\n#### 内连接\n- **隐式内连接**\n  ```sql\n  SELECT 字段列表 FROM 表1, 表2 WHERE 条件;\n  ```\n\n- **显示内连接**\n  ```sql\n  SELECT 字段列表 FROM 表1 [INNER] JOIN 表2 ON 连接条件;\n  ```\n\n#### 自连接\n自连接是将一张表视为两张表，设置表别名进行查询。\n\n---\n\n## 事务\n\n### 概念\n事务是一组操作的集合，是一个不可分割的工作单位。  \n事务中的所有操作要么全部成功，要么全部失败。\n\n### 事务操作\n- **查看/设置事务提交方式**\n  ```sql\n  SELECT @@autocommit; -- 1 表示自动提交，0 表示手动提交\n\n  SET @@autocommit = 0;\n  ```\n\n- **开启事务**\n  ```sql\n  START TRANSACTION;\n  ```\n\n- **提交事务**\n  ```sql\n  COMMIT;\n  ```\n\n- **回滚事务**\n  ```sql\n  ROLLBACK;\n  ```\n\n### 事务并发问题\n1. **脏读**: 读取到未提交的数据。\n2. **不可重复读**: 一次事务中，数据多次读取结果不同。\n3. **幻读**: 一次事务中新增或删除数据导致总记录数不一致。\n\n### 事务隔离级别\n| 隔离级别           | 脏读 | 不可重复读 | 幻读 |\n|--------------------|-------|------------|-------|\n| Read uncommitted  | 会    | 会         | 会    |\n| Read committed    | 不会  | 会         | 会    |\n| Repeatable Read   | 不会  | 不会       | 会    |\n| Serializable      | 不会  | 不会       | 不会  |\n\n- **查看事务隔离级别**\n  ```sql\n  SELECT @@TRANSACTION_ISOLATION;\n  ```\n\n- **设置事务隔离级别**\n  ```sql\n  SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE};\n  ```","tags":["数据库","SQL"],"categories":["SQL"]},{"title":"四种SQL语句","url":"/2024/11/28/四种SQL语句/","content":"\n# 数据库操作与命令总结\n\n## DLL 定义\n\n### 数据库操作\n```sql\nSHOW DATABASES;\n\nCREATE DATABASE 数据库表名;\n\nUSE 数据库名;\n\nSELECT DATABASE(); -- 展示当前所在的数据库\n\nDROP DATABASE 数据库名; -- 删除数据库\n```\n\n### 表操作\n```sql\nSHOW TABLES;\n\nCREATE TABLE 表名 (字段 字段类型, ...);\n\nDESC 表名; -- 查看当前的表结构\n\nSHOW CREATE TABLE 表名;\n\nDROP TABLE 表名;\n```\n\n### 表操作 - 修改\n- **添加字段**\n  ```sql\n  ALTER TABLE 表名 ADD 字段名 类型(长度) [COMMENT 注释] [约束];\n  ```\n\n- **修改字段类型**\n  ```sql\n  ALTER TABLE 表名 MODIFY 字段名 数据类型(长度);\n  ```\n\n- **修改字段名和字段类型**\n  ```sql\n  ALTER TABLE 表名 CHANGE 旧字段名 新字段名 类型(长度) [COMMENT 注释] [约束];\n  ```\n\n- **删除字段**\n  ```sql\n  ALTER TABLE 表名 DROP 字段名;\n  ```\n\n- **修改表名**\n  ```sql\n  ALTER TABLE 表名 RENAME TO 新表名;\n  ```\n\n- **删除表**\n  ```sql\n  DROP TABLE [IF EXISTS] 表名;\n  ```\n\n- **删除并重新创建表**\n  ```sql\n  TRUNCATE TABLE 表名;\n  ```\n\n---\n\n## DML 修改\n\n### 添加数据 (INSERT)\n```sql\n-- 给指定字段添加数据\nINSERT INTO 表名 (字段名1, 字段名2...) VALUES (值1, 值2, ...);\n\n-- 给全部字段添加数据\nINSERT INTO 表名 VALUES (值1, 值2, ...);\n\n-- 批量添加数据\nINSERT INTO 表名 (字段名1, 字段名...) VALUES (值1, 值2...), (值1, 值2...), ...;\n\nINSERT INTO 表名 VALUES (值1, 值2...), (值1, 值2...), ...;\n```\n\n### 修改数据 (UPDATE)\n```sql\nUPDATE 表名 SET 字段名1 = 值1, 字段名2 = 值2, ... [WHERE 条件];\n```\n\n### 删除数据 (DELETE)\n```sql\nDELETE FROM 表名 [WHERE 条件];\n-- 注意: 不能删除某个字段的值, 要用 UPDATE;\n```\n\n---\n\n## DQL 查询\n\n### 基本查询\n```sql\n-- 查询多个字段\nSELECT 字段1, 字段2, ... FROM 表名;\n\nSELECT * FROM 表名;\n\n-- 设置别名\nSELECT 字段1 [AS 别名1], 字段2 [AS 别名2], ... FROM 表名;\n\n-- 去除重复记录\nSELECT DISTINCT 字段列表 FROM 表名;\n```\n\n### 条件查询\n```sql\nSELECT 字段列表 FROM 表名 WHERE 条件列表;\n-- 常见条件:\n<> 或 !=      -- 不等于\nBETWEEN ... AND ... -- 在范围内 (含最大最小值)\nIN (...)      -- 在列表中\nLIKE 占位符    -- 模糊匹配 (_匹配单字符, %匹配任意字符)\nIS NULL       -- 是 NULL\n```\n\n### 聚合函数\n```sql\n-- 常见聚合函数:\nCOUNT -- 统计数量\nMAX   -- 最大值\nMIN   -- 最小值\nAVG   -- 平均值\nSUM   -- 求和\n\n-- 使用示例:\nSELECT 聚合函数(字段列表) FROM 表名;\n```\n\n### 分组查询\n```sql\nSELECT 字段列表 FROM 表名 [WHERE 条件] GROUP BY 分组字段名 [HAVING 分组后过滤条件];\n```\n\n### 排序查询\n```sql\nSELECT 字段列表 FROM 表名 ORDER BY 字段1 排序方式1, 字段2 排序方式2;\n-- 排序方式:\n-- ASC : 升序 (默认值)\n-- DESC: 降序\n```\n\n### 分页查询\n```sql\nSELECT 字段列表 FROM 表名 LIMIT 起始索引, 查询记录数;\n```\n\n---\n\n## DCL 控制\n\n### 用户管理\n```sql\n-- 查询用户\nUSE mysql;\nSELECT * FROM user;\n\n-- 创建用户\nCREATE USER '用户名'@'主机名' IDENTIFIED BY '密码';\n\n-- 修改用户密码\nALTER USER '用户名'@'主机名' IDENTIFIED WITH mysql_native_password BY '新密码';\n\n-- 删除用户\nDROP USER '用户名'@'主机名';\n```\n\n### 权限控制\n- **常用权限**\n  ```text\n  ALL, ALL PRIVILEGES -- 所有权限\n  SELECT              -- 查询数据\n  INSERT              -- 插入数据\n  UPDATE              -- 修改数据\n  DELETE              -- 删除数据\n  ALTER               -- 修改表\n  DROP                -- 删除数据库/表/视图\n  CREATE              -- 创建数据库/表\n  ```\n\n- **查询权限**\n  ```sql\n  SHOW GRANTS FOR '用户名'@'主机名';\n  ```\n\n- **授予权限**\n  ```sql\n  GRANT 权限列表 ON 数据库.表名 TO '用户名'@'主机名';\n  ```\n\n- **撤销权限**\n  ```sql\n  REVOKE 权限列表 ON 数据库.表名 FROM '用户名'@'主机名';\n  ```\n  > 注意: 数据库.表名 可以使用通配符 `*.*` 表示所有数据库的所有表。","tags":["数据库","SQL"],"categories":["SQL"]},{"title":"FFmpeg 基本命令","url":"/2024/11/28/FFmpeg 基本命令/","content":"\n# FFmpeg 使用命令\n\n## 查询可用设备\n```bash\nffmpeg -list_devices true -f dshow -i dummy\n```\n\n---\n\n## 录制\n\n- **摄像头录制**\n  ```bash\n  ffmpeg -f dshow -r 30 -i video=\"USB2.0 HD UVC WebCam\" output.yuv\n  ```\n\n- **麦克风录制**\n  ```bash\n  ffmpeg -f dshow -i audio=\"麦克风阵列 (Realtek(R) Audio)\" output.pcm\n  ```\n\n---\n\n## 播放\n\n- **播放视频**\n  ```bash\n  ffplay -i output.yuv -video_size 1280x720 -framerate 30 -pixel_format yuvj422p\n  ```\n\n- **播放音频**\n  ```bash\n  ffplay -i output.pcm -ar 48000 -f s16le\n  ```\n\n---\n\n## 处理原始数据\n\n- **提取 YUV 视频数据**\n  ```bash\n  ffmpeg -i input.mp4 -an -c:v rawvideo -pixel_format yuv420p out.yuv\n  ```\n\n- **提取 PCM 音频数据**\n  ```bash\n  ffmpeg -i input.mp4 -vn -ar 48000 -channels 2 -f s16le output.pcm\n  ```\n\n---\n\n## 视频滤镜\n\n- **裁剪视频宽高各减 200**\n  ```bash\n  ffmpeg -i input.mp4 -vf crop=in_w-200:in_h-200 -c:v libx264 -c:a copy output.mp4\n  ```\n\n- **从指定时间开始裁剪 10 秒**\n  ```bash\n  ffmpeg -i input.mp4 -ss 00:00:00 -t 10 output.mp4\n  ```\n\n- **拼接多个视频**\n  ```bash\n  ffmpeg -f concat -i input.txt output.mp4\n  ```\n  > `input.txt` 文件内容示例：\n  > ```txt\n  > file 'file1.mp4'\n  > file 'file2.mp4'\n  > ```\n\n---\n\n## 图片与视频转换\n\n- **将视频转换为图片**\n  ```bash\n  ffmpeg -i input.mp4 -r 1 -f image2 image-%3d.jpeg\n  ```\n\n- **将图片转换为视频**\n  ```bash\n  ffmpeg -i image-%3d.jpeg out.mp4\n  ```\n\n# FFprobe 使用命令\n\n## 获取视频帧信息\n  ```bash\n  ffprobe -show_frames -select_streams v:0 -print_format json output.mp4\n  ```\n## 学习中...","tags":["FFmpeg"],"categories":["音视频"]}]